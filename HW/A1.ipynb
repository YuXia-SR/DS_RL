{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS-UA 301: NLP & RL -- Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this notebook, we will explore and practice several foundational concepts and techniques in Natural Language Processing, specifically in the \"pre-processing\" stage: \n",
    "+ Finding and getting a corpus up and running in your Jupyter notebook\n",
    "+ Cleaning & tokenizing text data\n",
    "+ POS tagging \n",
    "+ Stemming & lemmatization\n",
    "\n",
    "We'll also *start to* think about approaching our NLP work from a hypothesis-driven approach. This will help for the project, as well as life in general, as we've elaborated ad nauseum in lecture already (and, fair warning, will continue to do).\n",
    "\n",
    "Some tips: \n",
    "+ You may use any data you like (built-in NLTK or something else!) as long as you are able to complete all the prompts below.\n",
    "+ You're also welcome to try the assignment with both an NLTK corpus *and* something you collect yourself! No extra credit, just extra learning :).\n",
    "+ While we will be grading you for correctness and completeness; i.e., you must correctly complete the specific tasks we ask of you, there is rarely a single \"right answer\" in much of data science, NLP very much included, when it comes to *which* strategy to use.\n",
    "+ What this means for you is: for example, while we do need you to, e.g., remove all capital letters if we ask you to, for questions about your decision-making about whether or not removing capital letters is appropriate, we care more that you thoughtfully and transparently explain your reasoning, rather than worrying whether it's \"right\" to keep them or not. \n",
    "+ Of course, we want you to use your best judgement, but usually there isn't a single correct answer out there floating around for questions about what method you use. It's about balancing tradeoffs, and we want to see that you understand the tradeoffs.\n",
    "+ In other words, to loosely quote a wise data scientist (not me, ha ha ha (but really it isn't)): **Keep the precision but let go of perfectionism!**\n",
    "+ Also, 2-3 sentences (at most!) for the non-code questions should generally be enough. No need for essays!\n",
    "\n",
    "Finally, each question is worth 1 whole point, for 45 points total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding some text you'd like to represent as data\n",
    "\n",
    "This step may be less trivial an exercise than it seems at first glance. As with structured, numeric data, finding data that's both usable *and* addresses the question you're interested in is often no small feat.\n",
    "\n",
    "If you go the NLP route for the project, you'll eventually need to use your own data (alas, NLTK library data is not allowed for that). But for this assignment, especially if it's your first go at NLP, you're more than welcome to use something built in so you can focus on the techniques.\n",
    "\n",
    "That said, it may make your life (eventually) easier if you start exploring for and with your own data now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What text will you be investigating in this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to scrap some sample blogs from the website: https://towardsdatascience.com/, a blog column mainly focus on topics in data science.\n",
    "\n",
    "For this homework, I'm going to use this sample article: https://towardsdatascience.com/10-highly-probable-data-scientist-interview-questions-fd83f7414760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Why did you choose this text? What do you hope to learn from it? (Even if you're working with an NLTK corpus, surely you have a reason you were drawn to one!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm personally interested in how data literacy could be educated through free resource online. After analyzing such corpus, I hope to study what are some popular topics, and potentially popular techniques to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Imagine, hypothetically, that you needed to form a hypothesis about this data (you won't have to test it in this assignment). What are three hypotheses you could test using this data? (They can be three related hypotheses, or not! You can also imagine that you would eventaully add other data if needed, or not.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H1: the word \\\"python\\\" appears relatively frequent than other type of articles, for example news\n",
    "\n",
    "H2: There\\'re more capitalized words(standing for proper noun of models, etc.) than other type of articles\n",
    "\n",
    "H3: There\\'re more digits and mathematical expression here than other type of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) For *one* of the hypotheses above, how would you test it, and how would you know if your hypothesis is wrong? In other words, what sort of result would disprove it in the context of your study? (Again, you don't have to test anything in this assignment, but (obviously) this is practice for the project!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first hypothesis as an example. I will start from collecting a bunch of towardsdatascience blogs and news, and compute the average frequency of word \\\"python\\\" for each type of article, i.e. freq(\\\"python\\\") = count(\\\"python\\\") / len(document). If the hypothesis is true, we expect a higher average frequency of word \\\"python\\\" in towardsdatascience blogs. \n",
    "\n",
    "We could directly take the original texts and compute the frequency, or as a potential improvement, we could also first clean the text by removing punctuation, stop words and so on to represent how much does the word \\\"python\\\" take up on meaningful information of each type of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Ok, it's time to get it ready to go in Python. Import, load, or do whatever's necessary so your text is workable in one document here (you can have more than one document, but it's not needed for this assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popularity of data science attracts a lot of people from a wide range of professions to make a career change with the goal of becoming a data scientist.Despite the high demand for data scientists, it is a highly challenging task to find your first job. Unless you have a solid prior job experience, interviews are where you can show you skills and impress your potential employer.Data science is an interdisciplinary field which covers a broad range of topics and concepts. Thus, the number of questions that you might be asked at an interview is very high.However, there are some questions about the fundamentals in data science and machine learning. These are the ones you do not want to miss. In this article, we will go over 10 questions that are likely to be asked at a data scientist interview.The questions are grouped into 3 main categories which are machine learning, Python, and SQL. I will try to provide a brief answer for each question. However, I suggest reading or studying each one in more detail afterwards.Overfitting in machine learning occurs when your model is not generalized well. The model is too focused on the training set. It captures a lot of detail or even noise in the training set. Thus, it fails to capture the general trend or the relationships in the data. If a model is too complex compared to the data, it will probably be overfitting.A strong indicator of overfitting is the high difference between the accuracy of training and test sets. Overfit models usually have very high accuracy on the training set but the test accuracy is usually unpredictable and much lower than the training accuracy.We can reduce overfitting by making the model more generalized which means it should be more focused on the general trend rather than specific details.If it is possible, collecting more data is an efficient way to reduce overfitting. You will be giving more juice to the model so it will have more material to learn from. Data is always valuable especially for machine learning models.Another method to reduce overfitting is to reduce the complexity of the model. If a model is too complex for a given task, it will likely result in overfitting. In such cases, we should look for simpler models.We have mentioned that the main reason for overfitting is a model being more complex than necessary. Regularization is a method for reducing the model complexity.It does so by penalizing higher terms in the model. With the addition of a regularization term, the model tries to minimize both loss and complexity.Two main types of regularization are L1 and L2 regularization. L1 regularization subtracts a small amount from the weights of uninformative features at each iteration. Thus, it causes these weights to eventually become zero.On the other hand, L2 regularization removes a small percentage from the weights at each iteration. These weights will get closer to zero but never actually become 0.Both are machine learning tasks. Classification is a supervised learning task so we have labelled observations (i.e. data points). We train a model with labelled data and expect it to predict the labels of new data.For instance, spam email detection is a classification task. We provide a model with several emails marked as spam or not spam. After the model is trained with those emails, it will evaluate the new emails appropriately.Clustering is an unsupervised learning task so the observations do not have any labels. The model is expected to evaluate the observations and group them into clusters. Similar observations are placed into the same cluster.In the optimal case, the observations in the same cluster are as close to each other as possible and the different clusters are as far apart as possible. An example of a clustering task would be grouping customers based on their shopping behavior.The built-in data structures are of crucial importance. Thus, you should be familiar with what they are and how to interact with them. List, dictionary, set, and tuple are 4 main built-in data structures in Python.The main difference between lists and tuples is mutability. Lists are mutable so we can manipulate them by adding or removing items.On the other hand, tuples are immutable. Although we can access each element in a tuple, we cannot modify its content.One important point to mention here is that although tuples are immutable, they can contain mutable elements such as lists or sets.Let’s do an example to demonstrate the main difference between lists and sets.As we notice in the resulting objects, the list contains all the characters in the string whereas the set only contains unique values.Another difference is that the characters in the list are ordered based on their location in the string. However, there is no order associated with the characters in the set.Here is a table that summarizes the main characteristics of lists, tuples, and sets.A dictionary in Python is a collection of key-value pairs. It is similar to a list in the sense that each item in a list has an associated index starting from 0.In a dictionary, we have keys as the index. Thus, we can access a value by using its key.The keys in a dictionary are unique which makes sense because they act like an address for the values.SQL is an extremely important skill for data scientists. There are quite a number of companies that store their data in a relational database. SQL is what is needed to interact with relational databases.You will probably be asked a question that involves writing a query to perform a specific task. You might also be asked a question about general database knowledge.Consider we have a sales table that contains daily sales quantities of products.Find the top 5 weeks in terms of total weekly sales quantities.We first extract the year and week information from the date column and then use it in the aggregation. The sum function is used to calculate the total sales quantities.In the same sales table, find the number of unique items that are sold each month.These terms are related to database schema design. Normalization and denormalization aim to optimize different metrics.The goal of normalization is to reduce data redundancy and inconsistency by increasing the number of tables. On the other hand, denormalization aims to speed up the query execution. Denormalization decreases the number of tables but at the same time, it adds some redundancy.It is a challenging task to become a data scientist. It requires time, effort, and dedication. Without having prior job experience, the process gets harder.Interviews are very important to demonstrate your skills. In this article, we have covered 10 questions that you are likely to encounter in a data scientist interview.Thank you for reading. Please let me know if you have any feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) How many words are we working with here? (I.e., how many are in the corpus you'll be using in this assignment?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1165 words, according to the word count function in Microsoft Word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Hooray! You're ready to start pre-processing. First, consider capitalizations. Before we do anything with them, think about what you'd like to do with this data, and what sort of data it is. Do you think you'll want to keep the capitalizations, or change everything to lowercase? Or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will change everything to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Briefly explain your answer to 2a above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we focus on analyzing the content instead of the text format using in the blog, changing the whole text into lowercase is good and straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For fun(!), regardless of your answer to 2a, write some code to remove all capitalizations. (If you want to keep the capitalization in later stages, just comment it out after you confirm it works, but leave the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The popularity of data science attracts a lot of people from a wide range of professions to make a career change with the goal of becoming a data scientist.Despite the high demand for data scientists, it is a highly challenging task to find your first job. Unless you have a solid prior job experience, interviews are where you can show you skills and impress your potential employer.Data science is an interdisciplinary field which covers a broad range of topics and concepts. Thus, the number of questions that you might be asked at an interview is very high.However, there are some questions about the fundamentals in data science and machine learning. These are the ones you do not want to miss. In this article, we will go over 10 questions that are likely to be asked at a data scientist interview.The questions are grouped into 3 main categories which are machine learning, Python, and SQL. I will try to provide a brief answer for each question. However, I suggest reading or studying each one in more detail afterwards.Overfitting in machine learning occurs when your model is not generalized well. The model is too focused on the training set. It captures a lot of detail or even noise in the training set. Thus, it fails to capture the general trend or the relationships in the data. If a model is too complex compared to the data, it will probably be overfitting.A strong indicator of overfitting is the high difference between the accuracy of training and test sets. Overfit models usually have very high accuracy on the training set but the test accuracy is usually unpredictable and much lower than the training accuracy.We can reduce overfitting by making the model more generalized which means it should be more focused on the general trend rather than specific details.If it is possible, collecting more data is an efficient way to reduce overfitting. You will be giving more juice to the model so it will have more material to learn from. Data is always valuable especially for machine learning models.Another method to reduce overfitting is to reduce the complexity of the model. If a model is too complex for a given task, it will likely result in overfitting. In such cases, we should look for simpler models.We have mentioned that the main reason for overfitting is a model being more complex than necessary. Regularization is a method for reducing the model complexity.It does so by penalizing higher terms in the model. With the addition of a regularization term, the model tries to minimize both loss and complexity.Two main types of regularization are L1 and L2 regularization. L1 regularization subtracts a small amount from the weights of uninformative features at each iteration. Thus, it causes these weights to eventually become zero.On the other hand, L2 regularization removes a small percentage from the weights at each iteration. These weights will get closer to zero but never actually become 0.Both are machine learning tasks. Classification is a supervised learning task so we have labelled observations (i.e. data points). We train a model with labelled data and expect it to predict the labels of new data.For instance, spam email detection is a classification task. We provide a model with several emails marked as spam or not spam. After the model is trained with those emails, it will evaluate the new emails appropriately.Clustering is an unsupervised learning task so the observations do not have any labels. The model is expected to evaluate the observations and group them into clusters. Similar observations are placed into the same cluster.In the optimal case, the observations in the same cluster are as close to each other as possible and the different clusters are as far apart as possible. An example of a clustering task would be grouping customers based on their shopping behavior.The built-in data structures are of crucial importance. Thus, you should be familiar with what they are and how to interact with them. List, dictionary, set, and tuple are 4 main built-in data structures in Python.The main difference between lists and tuples is mutability. Lists are mutable so we can manipulate them by adding or removing items.On the other hand, tuples are immutable. Although we can access each element in a tuple, we cannot modify its content.One important point to mention here is that although tuples are immutable, they can contain mutable elements such as lists or sets.Let’s do an example to demonstrate the main difference between lists and sets.As we notice in the resulting objects, the list contains all the characters in the string whereas the set only contains unique values.Another difference is that the characters in the list are ordered based on their location in the string. However, there is no order associated with the characters in the set.Here is a table that summarizes the main characteristics of lists, tuples, and sets.A dictionary in Python is a collection of key-value pairs. It is similar to a list in the sense that each item in a list has an associated index starting from 0.In a dictionary, we have keys as the index. Thus, we can access a value by using its key.The keys in a dictionary are unique which makes sense because they act like an address for the values.SQL is an extremely important skill for data scientists. There are quite a number of companies that store their data in a relational database. SQL is what is needed to interact with relational databases.You will probably be asked a question that involves writing a query to perform a specific task. You might also be asked a question about general database knowledge.Consider we have a sales table that contains daily sales quantities of products.Find the top 5 weeks in terms of total weekly sales quantities.We first extract the year and week information from the date column and then use it in the aggregation. The sum function is used to calculate the total sales quantities.In the same sales table, find the number of unique items that are sold each month.These terms are related to database schema design. Normalization and denormalization aim to optimize different metrics.The goal of normalization is to reduce data redundancy and inconsistency by increasing the number of tables. On the other hand, denormalization aims to speed up the query execution. Denormalization decreases the number of tables but at the same time, it adds some redundancy.It is a challenging task to become a data scientist. It requires time, effort, and dedication. Without having prior job experience, the process gets harder.Interviews are very important to demonstrate your skills. In this article, we have covered 10 questions that you are likely to encounter in a data scientist interview.Thank you for reading. Please let me know if you have any feedback.\"\"\"\n",
    "text_lower = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) For even more(!) fun, regardless of your answer to 2a, write some code to instead remove only capitalizations that appear at the beginning of each sentence. (Again, if this doesn't suit your overall analysis, just comment it out after you're done.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the following result, we could find terms like \\\"SQL\\\", \\\"L1 regularization\\\" are kept capitalized at the first letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the popularity of data science attracts a lot of people from a wide range of professions to make a career change with the goal of becoming a data scientist.despite the high demand for data scientists, it is a highly challenging task to find your first job. unless you have a solid prior job experience, interviews are where you can show you skills and impress your potential employer.data science is an interdisciplinary field which covers a broad range of topics and concepts. thus, the number of questions that you might be asked at an interview is very high.however, there are some questions about the fundamentals in data science and machine learning. these are the ones you do not want to miss. in this article, we will go over 10 questions that are likely to be asked at a data scientist interview.the questions are grouped into 3 main categories which are machine learning, python, and SQL. i will try to provide a brief answer for each question. however, i suggest reading or studying each one in more detail afterwards.overfitting in machine learning occurs when your model is not generalized well. the model is too focused on the training set. it captures a lot of detail or even noise in the training set. thus, it fails to capture the general trend or the relationships in the data. if a model is too complex compared to the data, it will probably be overfitting.a strong indicator of overfitting is the high difference between the accuracy of training and test sets. overfit models usually have very high accuracy on the training set but the test accuracy is usually unpredictable and much lower than the training accuracy.we can reduce overfitting by making the model more generalized which means it should be more focused on the general trend rather than specific details.if it is possible, collecting more data is an efficient way to reduce overfitting. you will be giving more juice to the model so it will have more material to learn from. data is always valuable especially for machine learning models.another method to reduce overfitting is to reduce the complexity of the model. if a model is too complex for a given task, it will likely result in overfitting. in such cases, we should look for simpler models.we have mentioned that the main reason for overfitting is a model being more complex than necessary. regularization is a method for reducing the model complexity.it does so by penalizing higher terms in the model. with the addition of a regularization term, the model tries to minimize both loss and complexity.two main types of regularization are L1 and L2 regularization. l1 regularization subtracts a small amount from the weights of uninformative features at each iteration. thus, it causes these weights to eventually become zero.on the other hand, l2 regularization removes a small percentage from the weights at each iteration. these weights will get closer to zero but never actually become 0.both are machine learning tasks. classification is a supervised learning task so we have labelled observations (i.e. data points). we train a model with labelled data and expect it to predict the labels of new data.for instance, spam email detection is a classification task. we provide a model with several emails marked as spam or not spam. after the model is trained with those emails, it will evaluate the new emails appropriately.clustering is an unsupervised learning task so the observations do not have any labels. the model is expected to evaluate the observations and group them into clusters. similar observations are placed into the same cluster.in the optimal case, the observations in the same cluster are as close to each other as possible and the different clusters are as far apart as possible. an example of a clustering task would be grouping customers based on their shopping behavior.the built-in data structures are of crucial importance. thus, you should be familiar with what they are and how to interact with them. list, dictionary, set, and tuple are 4 main built-in data structures in Python.the main difference between lists and tuples is mutability. lists are mutable so we can manipulate them by adding or removing items.on the other hand, tuples are immutable. although we can access each element in a tuple, we cannot modify its content.one important point to mention here is that although tuples are immutable, they can contain mutable elements such as lists or sets.let’s do an example to demonstrate the main difference between lists and sets.as we notice in the resulting objects, the list contains all the characters in the string whereas the set only contains unique values.another difference is that the characters in the list are ordered based on their location in the string. however, there is no order associated with the characters in the set.here is a table that summarizes the main characteristics of lists, tuples, and sets.a dictionary in Python is a collection of key-value pairs. it is similar to a list in the sense that each item in a list has an associated index starting from 0.in a dictionary, we have keys as the index. thus, we can access a value by using its key.the keys in a dictionary are unique which makes sense because they act like an address for the values.sQL is an extremely important skill for data scientists. there are quite a number of companies that store their data in a relational database. sQL is what is needed to interact with relational databases.you will probably be asked a question that involves writing a query to perform a specific task. you might also be asked a question about general database knowledge.consider we have a sales table that contains daily sales quantities of products.find the top 5 weeks in terms of total weekly sales quantities.we first extract the year and week information from the date column and then use it in the aggregation. the sum function is used to calculate the total sales quantities.in the same sales table, find the number of unique items that are sold each month.these terms are related to database schema design. normalization and denormalization aim to optimize different metrics.the goal of normalization is to reduce data redundancy and inconsistency by increasing the number of tables. on the other hand, denormalization aims to speed up the query execution. denormalization decreases the number of tables but at the same time, it adds some redundancy.it is a challenging task to become a data scientist. it requires time, effort, and dedication. without having prior job experience, the process gets harder.interviews are very important to demonstrate your skills. in this article, we have covered 10 questions that you are likely to encounter in a data scientist interview.thank you for reading. please let me know if you have any feedback.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get puncutation\n",
    "import string\n",
    "#punctuation = string.punctuation\n",
    "is_punct = True\n",
    "text_lowFirst = \"\"\n",
    "for letter in text:\n",
    "    if is_punct:\n",
    "        if letter in string.ascii_letters:\n",
    "            text_lowFirst += letter.lower()\n",
    "        else:\n",
    "            text_lowFirst += letter\n",
    "    else:\n",
    "        text_lowFirst += letter\n",
    "    if letter == \" \":\n",
    "        continue\n",
    "    is_punct = letter in string.punctuation\n",
    "text_lowFirst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) So far we've tried the capitalization options we discussed in lecture. Now, think up your own capitalization rule (anything at all is fine, though try to think of what might be most useful for your text and goals). What rule will you implement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering down the first word of a sentence is a good idea for capitalization rule. However, there may be possible situtation, for example, \\\"SQL is a good tool to manage relational database and execute various operations on data.\\\" We cannot directly lower the first letter like \\\"sQL\\\".\n",
    "\n",
    "There is plenty capitalized proper nouns using in the field of Data Science. A potential solution is to maintain a dictionary of these nouns. However, we barely see these words (even in lowercase) pointing to another kind of meaning, thus we could ignore this step. Not mentioned that it's time-consuming and labor consuming to maintain a updated dictionary as new terms in data science emerge pretty fast recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Go on, implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The popularity of data science attracts a lot of people from a wide range of professions to make a career change with the goal of becoming a data scientist.Despite the high demand for data scientists, it is a highly challenging task to find your first job. Unless you have a solid prior job experience, interviews are where you can show you skills and impress your potential employer.Data science is an interdisciplinary field which covers a broad range of topics and concepts. Thus, the number of questions that you might be asked at an interview is very high.However, there are some questions about the fundamentals in data science and machine learning. These are the ones you do not want to miss. In this article, we will go over 10 questions that are likely to be asked at a data scientist interview.The questions are grouped into 3 main categories which are machine learning, Python, and SQL. I will try to provide a brief answer for each question. However, I suggest reading or studying each one in more detail afterwards.Overfitting in machine learning occurs when your model is not generalized well. The model is too focused on the training set. It captures a lot of detail or even noise in the training set. Thus, it fails to capture the general trend or the relationships in the data. If a model is too complex compared to the data, it will probably be overfitting.A strong indicator of overfitting is the high difference between the accuracy of training and test sets. Overfit models usually have very high accuracy on the training set but the test accuracy is usually unpredictable and much lower than the training accuracy.We can reduce overfitting by making the model more generalized which means it should be more focused on the general trend rather than specific details.If it is possible, collecting more data is an efficient way to reduce overfitting. You will be giving more juice to the model so it will have more material to learn from. Data is always valuable especially for machine learning models.Another method to reduce overfitting is to reduce the complexity of the model. If a model is too complex for a given task, it will likely result in overfitting. In such cases, we should look for simpler models.We have mentioned that the main reason for overfitting is a model being more complex than necessary. Regularization is a method for reducing the model complexity.It does so by penalizing higher terms in the model. With the addition of a regularization term, the model tries to minimize both loss and complexity.Two main types of regularization are L1 and L2 regularization. L1 regularization subtracts a small amount from the weights of uninformative features at each iteration. Thus, it causes these weights to eventually become zero.On the other hand, L2 regularization removes a small percentage from the weights at each iteration. These weights will get closer to zero but never actually become 0.Both are machine learning tasks. Classification is a supervised learning task so we have labelled observations (i.e. data points). We train a model with labelled data and expect it to predict the labels of new data.For instance, spam email detection is a classification task. We provide a model with several emails marked as spam or not spam. After the model is trained with those emails, it will evaluate the new emails appropriately.Clustering is an unsupervised learning task so the observations do not have any labels. The model is expected to evaluate the observations and group them into clusters. Similar observations are placed into the same cluster.In the optimal case, the observations in the same cluster are as close to each other as possible and the different clusters are as far apart as possible. An example of a clustering task would be grouping customers based on their shopping behavior.The built-in data structures are of crucial importance. Thus, you should be familiar with what they are and how to interact with them. List, dictionary, set, and tuple are 4 main built-in data structures in Python.The main difference between lists and tuples is mutability. Lists are mutable so we can manipulate them by adding or removing items.On the other hand, tuples are immutable. Although we can access each element in a tuple, we cannot modify its content.One important point to mention here is that although tuples are immutable, they can contain mutable elements such as lists or sets.Let’s do an example to demonstrate the main difference between lists and sets.As we notice in the resulting objects, the list contains all the characters in the string whereas the set only contains unique values.Another difference is that the characters in the list are ordered based on their location in the string. However, there is no order associated with the characters in the set.Here is a table that summarizes the main characteristics of lists, tuples, and sets.A dictionary in Python is a collection of key-value pairs. It is similar to a list in the sense that each item in a list has an associated index starting from 0.In a dictionary, we have keys as the index. Thus, we can access a value by using its key.The keys in a dictionary are unique which makes sense because they act like an address for the values.SQL is an extremely important skill for data scientists. There are quite a number of companies that store their data in a relational database. SQL is what is needed to interact with relational databases.You will probably be asked a question that involves writing a query to perform a specific task. You might also be asked a question about general database knowledge.Consider we have a sales table that contains daily sales quantities of products.Find the top 5 weeks in terms of total weekly sales quantities.We first extract the year and week information from the date column and then use it in the aggregation. The sum function is used to calculate the total sales quantities.In the same sales table, find the number of unique items that are sold each month.These terms are related to database schema design. Normalization and denormalization aim to optimize different metrics.The goal of normalization is to reduce data redundancy and inconsistency by increasing the number of tables. On the other hand, denormalization aims to speed up the query execution. Denormalization decreases the number of tables but at the same time, it adds some redundancy.It is a challenging task to become a data scientist. It requires time, effort, and dedication. Without having prior job experience, the process gets harder.Interviews are very important to demonstrate your skills. In this article, we have covered 10 questions that you are likely to encounter in a data scientist interview.Thank you for reading. Please let me know if you have any feedback.\"\"\"\n",
    "text_lower = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Now that you've explored some capitalization options, which one do you think is the \"best\" fit for your text and goals? (And here's where you can comment out anything you won't use going forward and then re-run what's left so it's in for the shape you want for the next questions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, I will choose to just lower down the whole text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) Why do you think the rule you chose in 2g is the most appropriate for your work? Give one strength and one weakness of the rule you've chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's the most single method to deal with capitalization, it is quick enough and could well serve my following analysis since I care about the topics covered most in data science blogs.\n",
    "\n",
    "The downside would be exactly the one I described before about proper nouns used in the blog. If we lower down the whole text, they may appear in lowercases formats  in my result rather than standard formats usually seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) We're going to go through a similar process for punctuation as we did with capitalization, but don't worry, we'll pick up the pace a bit. First, do you think you'll want punctuation, or not, or something in between? Briefly explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will removing punctuation since I'm not focusing on the usage of punctuation in blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Remove whatever punctuation you think is appropriate for your work. You may need to try a few versions. There's no need to show us anything but the final one in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_punct = ['\\'','\\\"','%','+','-','<','=','>','^','*','/','(',')','[',']',':']\n",
    "punct = string.punctuation\n",
    "for i in allowed_punct:\n",
    "    punct = punct.replace(i,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the popularity of data science attracts a lot of people from a wide range of professions to make a career change with the goal of becoming a data scientist despite the high demand for data scientists  it is a highly challenging task to find your first job  unless you have a solid prior job experience  interviews are where you can show you skills and impress your potential employer data science is an interdisciplinary field which covers a broad range of topics and concepts  thus  the number of questions that you might be asked at an interview is very high however  there are some questions about the fundamentals in data science and machine learning  these are the ones you do not want to miss  in this article  we will go over 10 questions that are likely to be asked at a data scientist interview the questions are grouped into 3 main categories which are machine learning  python  and sql  i will try to provide a brief answer for each question  however  i suggest reading or studying each one in more detail afterwards overfitting in machine learning occurs when your model is not generalized well  the model is too focused on the training set  it captures a lot of detail or even noise in the training set  thus  it fails to capture the general trend or the relationships in the data  if a model is too complex compared to the data  it will probably be overfitting a strong indicator of overfitting is the high difference between the accuracy of training and test sets  overfit models usually have very high accuracy on the training set but the test accuracy is usually unpredictable and much lower than the training accuracy we can reduce overfitting by making the model more generalized which means it should be more focused on the general trend rather than specific details if it is possible  collecting more data is an efficient way to reduce overfitting  you will be giving more juice to the model so it will have more material to learn from  data is always valuable especially for machine learning models another method to reduce overfitting is to reduce the complexity of the model  if a model is too complex for a given task  it will likely result in overfitting  in such cases  we should look for simpler models we have mentioned that the main reason for overfitting is a model being more complex than necessary  regularization is a method for reducing the model complexity it does so by penalizing higher terms in the model  with the addition of a regularization term  the model tries to minimize both loss and complexity two main types of regularization are l1 and l2 regularization  l1 regularization subtracts a small amount from the weights of uninformative features at each iteration  thus  it causes these weights to eventually become zero on the other hand  l2 regularization removes a small percentage from the weights at each iteration  these weights will get closer to zero but never actually become 0 both are machine learning tasks  classification is a supervised learning task so we have labelled observations (i e  data points)  we train a model with labelled data and expect it to predict the labels of new data for instance  spam email detection is a classification task  we provide a model with several emails marked as spam or not spam  after the model is trained with those emails  it will evaluate the new emails appropriately clustering is an unsupervised learning task so the observations do not have any labels  the model is expected to evaluate the observations and group them into clusters  similar observations are placed into the same cluster in the optimal case  the observations in the same cluster are as close to each other as possible and the different clusters are as far apart as possible  an example of a clustering task would be grouping customers based on their shopping behavior the built-in data structures are of crucial importance  thus  you should be familiar with what they are and how to interact with them  list  dictionary  set  and tuple are 4 main built-in data structures in python the main difference between lists and tuples is mutability  lists are mutable so we can manipulate them by adding or removing items on the other hand  tuples are immutable  although we can access each element in a tuple  we cannot modify its content one important point to mention here is that although tuples are immutable  they can contain mutable elements such as lists or sets let’s do an example to demonstrate the main difference between lists and sets as we notice in the resulting objects  the list contains all the characters in the string whereas the set only contains unique values another difference is that the characters in the list are ordered based on their location in the string  however  there is no order associated with the characters in the set here is a table that summarizes the main characteristics of lists  tuples  and sets a dictionary in python is a collection of key-value pairs  it is similar to a list in the sense that each item in a list has an associated index starting from 0 in a dictionary  we have keys as the index  thus  we can access a value by using its key the keys in a dictionary are unique which makes sense because they act like an address for the values sql is an extremely important skill for data scientists  there are quite a number of companies that store their data in a relational database  sql is what is needed to interact with relational databases you will probably be asked a question that involves writing a query to perform a specific task  you might also be asked a question about general database knowledge consider we have a sales table that contains daily sales quantities of products find the top 5 weeks in terms of total weekly sales quantities we first extract the year and week information from the date column and then use it in the aggregation  the sum function is used to calculate the total sales quantities in the same sales table  find the number of unique items that are sold each month these terms are related to database schema design  normalization and denormalization aim to optimize different metrics the goal of normalization is to reduce data redundancy and inconsistency by increasing the number of tables  on the other hand  denormalization aims to speed up the query execution  denormalization decreases the number of tables but at the same time  it adds some redundancy it is a challenging task to become a data scientist  it requires time  effort  and dedication  without having prior job experience  the process gets harder interviews are very important to demonstrate your skills  in this article  we have covered 10 questions that you are likely to encounter in a data scientist interview thank you for reading  please let me know if you have any feedback '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lower_nopunct = \"\"\n",
    "for letter in text_lower:\n",
    "    if letter in punct:\n",
    "        text_lower_nopunct += \" \"\n",
    "    else:\n",
    "        text_lower_nopunct += letter\n",
    "text_lower_nopunct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What punctuation rule did you land on? Why did you decide on this one? Briefly walk us through the options you considered, if any. (If it was crystal clear to you what you wanted to do with the punctuation, just explain why.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remove commonly seen puntuations with function of separating sentences, but keep puntuations used in mathematical expression or with function of providing explanation. \n",
    "\n",
    "This is a simple and fast appraoch that could already well prepare the text for my following analysis. There could be plenty mathematical expressions in blog post for education in data science, and I could completely keep them in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) This isn't *exactly* punctuation, but let's do it here: Go ahead and remove any other miscellaneous text, symbols, or tags, if there are any, that you don't need. If there aren't any, just say so! (You don't even have to explain why you're doing it (for once!). We *get* it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_wanted_phrase = ['.com', 'towardsdatascience']\n",
    "for ph in not_wanted_phrase:\n",
    "    text_lower_nopunct = text_lower_nopunct.replace(ph,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) It's time to talk stop words! Before we get into which stop word list to use or what other stop words there might be for your text, share what you think will be right for you: drop most \"standard\" stop words, keep them, add some stop words of your own, or some combination? Briefly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can drop most \"standard\" stop words here, because the posts are usually written in standard English with relatively formal grammar and phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) It's sometimes a little tricky to tell where the most useful line is between the benefit of simplification and the loss of substance when it comes to stop words in a particular text. Explore their usefulness for your text by first dropping all stop words from the NLTK stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Stopwords: 603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/yuxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['popularity',\n",
       " 'data',\n",
       " 'science',\n",
       " 'attracts',\n",
       " 'lot',\n",
       " 'people',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'professions',\n",
       " 'make',\n",
       " 'career',\n",
       " 'change',\n",
       " 'goal',\n",
       " 'becoming',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'despite',\n",
       " 'high',\n",
       " 'demand',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'highly',\n",
       " 'challenging',\n",
       " 'task',\n",
       " 'find',\n",
       " 'first',\n",
       " 'job',\n",
       " 'unless',\n",
       " 'solid',\n",
       " 'prior',\n",
       " 'job',\n",
       " 'experience',\n",
       " 'interviews',\n",
       " 'show',\n",
       " 'skills',\n",
       " 'impress',\n",
       " 'potential',\n",
       " 'employer',\n",
       " 'data',\n",
       " 'science',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'covers',\n",
       " 'broad',\n",
       " 'range',\n",
       " 'topics',\n",
       " 'concepts',\n",
       " 'thus',\n",
       " 'number',\n",
       " 'questions',\n",
       " 'might',\n",
       " 'asked',\n",
       " 'interview',\n",
       " 'high',\n",
       " 'however',\n",
       " 'questions',\n",
       " 'fundamentals',\n",
       " 'data',\n",
       " 'science',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ones',\n",
       " 'want',\n",
       " 'miss',\n",
       " 'article',\n",
       " 'go',\n",
       " '10',\n",
       " 'questions',\n",
       " 'likely',\n",
       " 'asked',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'interview',\n",
       " 'questions',\n",
       " 'grouped',\n",
       " '3',\n",
       " 'main',\n",
       " 'categories',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'python',\n",
       " 'sql',\n",
       " 'try',\n",
       " 'provide',\n",
       " 'brief',\n",
       " 'answer',\n",
       " 'question',\n",
       " 'however',\n",
       " 'suggest',\n",
       " 'reading',\n",
       " 'studying',\n",
       " 'one',\n",
       " 'detail',\n",
       " 'afterwards',\n",
       " 'overfitting',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'occurs',\n",
       " 'model',\n",
       " 'generalized',\n",
       " 'well',\n",
       " 'model',\n",
       " 'focused',\n",
       " 'training',\n",
       " 'set',\n",
       " 'captures',\n",
       " 'lot',\n",
       " 'detail',\n",
       " 'even',\n",
       " 'noise',\n",
       " 'training',\n",
       " 'set',\n",
       " 'thus',\n",
       " 'fails',\n",
       " 'capture',\n",
       " 'general',\n",
       " 'trend',\n",
       " 'relationships',\n",
       " 'data',\n",
       " 'model',\n",
       " 'complex',\n",
       " 'compared',\n",
       " 'data',\n",
       " 'probably',\n",
       " 'overfitting',\n",
       " 'strong',\n",
       " 'indicator',\n",
       " 'overfitting',\n",
       " 'high',\n",
       " 'difference',\n",
       " 'accuracy',\n",
       " 'training',\n",
       " 'test',\n",
       " 'sets',\n",
       " 'overfit',\n",
       " 'models',\n",
       " 'usually',\n",
       " 'high',\n",
       " 'accuracy',\n",
       " 'training',\n",
       " 'set',\n",
       " 'test',\n",
       " 'accuracy',\n",
       " 'usually',\n",
       " 'unpredictable',\n",
       " 'much',\n",
       " 'lower',\n",
       " 'training',\n",
       " 'accuracy',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'making',\n",
       " 'model',\n",
       " 'generalized',\n",
       " 'means',\n",
       " 'focused',\n",
       " 'general',\n",
       " 'trend',\n",
       " 'rather',\n",
       " 'specific',\n",
       " 'details',\n",
       " 'possible',\n",
       " 'collecting',\n",
       " 'data',\n",
       " 'efficient',\n",
       " 'way',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'giving',\n",
       " 'juice',\n",
       " 'model',\n",
       " 'material',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'always',\n",
       " 'valuable',\n",
       " 'especially',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'another',\n",
       " 'method',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'reduce',\n",
       " 'complexity',\n",
       " 'model',\n",
       " 'model',\n",
       " 'complex',\n",
       " 'given',\n",
       " 'task',\n",
       " 'likely',\n",
       " 'result',\n",
       " 'overfitting',\n",
       " 'cases',\n",
       " 'look',\n",
       " 'simpler',\n",
       " 'models',\n",
       " 'mentioned',\n",
       " 'main',\n",
       " 'reason',\n",
       " 'overfitting',\n",
       " 'model',\n",
       " 'complex',\n",
       " 'necessary',\n",
       " 'regularization',\n",
       " 'method',\n",
       " 'reducing',\n",
       " 'model',\n",
       " 'complexity',\n",
       " 'penalizing',\n",
       " 'higher',\n",
       " 'terms',\n",
       " 'model',\n",
       " 'addition',\n",
       " 'regularization',\n",
       " 'term',\n",
       " 'model',\n",
       " 'tries',\n",
       " 'minimize',\n",
       " 'loss',\n",
       " 'complexity',\n",
       " 'two',\n",
       " 'main',\n",
       " 'types',\n",
       " 'regularization',\n",
       " 'l1',\n",
       " 'l2',\n",
       " 'regularization',\n",
       " 'l1',\n",
       " 'regularization',\n",
       " 'subtracts',\n",
       " 'small',\n",
       " 'amount',\n",
       " 'weights',\n",
       " 'uninformative',\n",
       " 'features',\n",
       " 'iteration',\n",
       " 'thus',\n",
       " 'causes',\n",
       " 'weights',\n",
       " 'eventually',\n",
       " 'become',\n",
       " 'zero',\n",
       " 'hand',\n",
       " 'l2',\n",
       " 'regularization',\n",
       " 'removes',\n",
       " 'small',\n",
       " 'percentage',\n",
       " 'weights',\n",
       " 'iteration',\n",
       " 'weights',\n",
       " 'get',\n",
       " 'closer',\n",
       " 'zero',\n",
       " 'never',\n",
       " 'actually',\n",
       " 'become',\n",
       " '0',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'tasks',\n",
       " 'classification',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'task',\n",
       " 'labelled',\n",
       " 'observations',\n",
       " '(',\n",
       " 'e',\n",
       " 'data',\n",
       " 'points',\n",
       " ')',\n",
       " 'train',\n",
       " 'model',\n",
       " 'labelled',\n",
       " 'data',\n",
       " 'expect',\n",
       " 'predict',\n",
       " 'labels',\n",
       " 'new',\n",
       " 'data',\n",
       " 'instance',\n",
       " 'spam',\n",
       " 'email',\n",
       " 'detection',\n",
       " 'classification',\n",
       " 'task',\n",
       " 'provide',\n",
       " 'model',\n",
       " 'several',\n",
       " 'emails',\n",
       " 'marked',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'emails',\n",
       " 'evaluate',\n",
       " 'new',\n",
       " 'emails',\n",
       " 'appropriately',\n",
       " 'clustering',\n",
       " 'unsupervised',\n",
       " 'learning',\n",
       " 'task',\n",
       " 'observations',\n",
       " 'labels',\n",
       " 'model',\n",
       " 'expected',\n",
       " 'evaluate',\n",
       " 'observations',\n",
       " 'group',\n",
       " 'clusters',\n",
       " 'similar',\n",
       " 'observations',\n",
       " 'placed',\n",
       " 'cluster',\n",
       " 'optimal',\n",
       " 'case',\n",
       " 'observations',\n",
       " 'cluster',\n",
       " 'close',\n",
       " 'possible',\n",
       " 'different',\n",
       " 'clusters',\n",
       " 'far',\n",
       " 'apart',\n",
       " 'possible',\n",
       " 'example',\n",
       " 'clustering',\n",
       " 'task',\n",
       " 'would',\n",
       " 'grouping',\n",
       " 'customers',\n",
       " 'based',\n",
       " 'shopping',\n",
       " 'behavior',\n",
       " 'built-in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'crucial',\n",
       " 'importance',\n",
       " 'thus',\n",
       " 'familiar',\n",
       " 'interact',\n",
       " 'list',\n",
       " 'dictionary',\n",
       " 'set',\n",
       " 'tuple',\n",
       " '4',\n",
       " 'main',\n",
       " 'built-in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'python',\n",
       " 'main',\n",
       " 'difference',\n",
       " 'lists',\n",
       " 'tuples',\n",
       " 'mutability',\n",
       " 'lists',\n",
       " 'mutable',\n",
       " 'manipulate',\n",
       " 'adding',\n",
       " 'removing',\n",
       " 'items',\n",
       " 'hand',\n",
       " 'tuples',\n",
       " 'immutable',\n",
       " 'although',\n",
       " 'access',\n",
       " 'element',\n",
       " 'tuple',\n",
       " 'modify',\n",
       " 'content',\n",
       " 'one',\n",
       " 'important',\n",
       " 'point',\n",
       " 'mention',\n",
       " 'although',\n",
       " 'tuples',\n",
       " 'immutable',\n",
       " 'contain',\n",
       " 'mutable',\n",
       " 'elements',\n",
       " 'lists',\n",
       " 'sets',\n",
       " 'let',\n",
       " '’',\n",
       " 'example',\n",
       " 'demonstrate',\n",
       " 'main',\n",
       " 'difference',\n",
       " 'lists',\n",
       " 'sets',\n",
       " 'notice',\n",
       " 'resulting',\n",
       " 'objects',\n",
       " 'list',\n",
       " 'contains',\n",
       " 'characters',\n",
       " 'string',\n",
       " 'whereas',\n",
       " 'set',\n",
       " 'contains',\n",
       " 'unique',\n",
       " 'values',\n",
       " 'another',\n",
       " 'difference',\n",
       " 'characters',\n",
       " 'list',\n",
       " 'ordered',\n",
       " 'based',\n",
       " 'location',\n",
       " 'string',\n",
       " 'however',\n",
       " 'order',\n",
       " 'associated',\n",
       " 'characters',\n",
       " 'set',\n",
       " 'table',\n",
       " 'summarizes',\n",
       " 'main',\n",
       " 'characteristics',\n",
       " 'lists',\n",
       " 'tuples',\n",
       " 'sets',\n",
       " 'dictionary',\n",
       " 'python',\n",
       " 'collection',\n",
       " 'key-value',\n",
       " 'pairs',\n",
       " 'similar',\n",
       " 'list',\n",
       " 'sense',\n",
       " 'item',\n",
       " 'list',\n",
       " 'associated',\n",
       " 'index',\n",
       " 'starting',\n",
       " '0',\n",
       " 'dictionary',\n",
       " 'keys',\n",
       " 'index',\n",
       " 'thus',\n",
       " 'access',\n",
       " 'value',\n",
       " 'using',\n",
       " 'key',\n",
       " 'keys',\n",
       " 'dictionary',\n",
       " 'unique',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'act',\n",
       " 'like',\n",
       " 'address',\n",
       " 'values',\n",
       " 'sql',\n",
       " 'extremely',\n",
       " 'important',\n",
       " 'skill',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'quite',\n",
       " 'number',\n",
       " 'companies',\n",
       " 'store',\n",
       " 'data',\n",
       " 'relational',\n",
       " 'database',\n",
       " 'sql',\n",
       " 'needed',\n",
       " 'interact',\n",
       " 'relational',\n",
       " 'databases',\n",
       " 'probably',\n",
       " 'asked',\n",
       " 'question',\n",
       " 'involves',\n",
       " 'writing',\n",
       " 'query',\n",
       " 'perform',\n",
       " 'specific',\n",
       " 'task',\n",
       " 'might',\n",
       " 'also',\n",
       " 'asked',\n",
       " 'question',\n",
       " 'general',\n",
       " 'database',\n",
       " 'knowledge',\n",
       " 'consider',\n",
       " 'sales',\n",
       " 'table',\n",
       " 'contains',\n",
       " 'daily',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'products',\n",
       " 'find',\n",
       " 'top',\n",
       " '5',\n",
       " 'weeks',\n",
       " 'terms',\n",
       " 'total',\n",
       " 'weekly',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'first',\n",
       " 'extract',\n",
       " 'year',\n",
       " 'week',\n",
       " 'information',\n",
       " 'date',\n",
       " 'column',\n",
       " 'use',\n",
       " 'aggregation',\n",
       " 'sum',\n",
       " 'function',\n",
       " 'used',\n",
       " 'calculate',\n",
       " 'total',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'sales',\n",
       " 'table',\n",
       " 'find',\n",
       " 'number',\n",
       " 'unique',\n",
       " 'items',\n",
       " 'sold',\n",
       " 'month',\n",
       " 'terms',\n",
       " 'related',\n",
       " 'database',\n",
       " 'schema',\n",
       " 'design',\n",
       " 'normalization',\n",
       " 'denormalization',\n",
       " 'aim',\n",
       " 'optimize',\n",
       " 'different',\n",
       " 'metrics',\n",
       " 'goal',\n",
       " 'normalization',\n",
       " 'reduce',\n",
       " 'data',\n",
       " 'redundancy',\n",
       " 'inconsistency',\n",
       " 'increasing',\n",
       " 'number',\n",
       " 'tables',\n",
       " 'hand',\n",
       " 'denormalization',\n",
       " 'aims',\n",
       " 'speed',\n",
       " 'query',\n",
       " 'execution',\n",
       " 'denormalization',\n",
       " 'decreases',\n",
       " 'number',\n",
       " 'tables',\n",
       " 'time',\n",
       " 'adds',\n",
       " 'redundancy',\n",
       " 'challenging',\n",
       " 'task',\n",
       " 'become',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'requires',\n",
       " 'time',\n",
       " 'effort',\n",
       " 'dedication',\n",
       " 'without',\n",
       " 'prior',\n",
       " 'job',\n",
       " 'experience',\n",
       " 'process',\n",
       " 'gets',\n",
       " 'harder',\n",
       " 'interviews',\n",
       " 'important',\n",
       " 'demonstrate',\n",
       " 'skills',\n",
       " 'article',\n",
       " 'covered',\n",
       " '10',\n",
       " 'questions',\n",
       " 'likely',\n",
       " 'encounter',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'interview',\n",
       " 'thank',\n",
       " 'reading',\n",
       " 'please',\n",
       " 'let',\n",
       " 'know',\n",
       " 'feedback']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) How do you think the NLTK stop words did? Do you think removing them is generally helpful for your analysis? Why or why not? \n",
    "\n",
    "(By the way, I know you're not doing a *full* analysis, so it's ok if at this point you're thinking -- hang on, well, if eventually I want to do X, I wouldn't need stop words, but if I wanted to do Y, I probably would -- feel free to tell us that, or just pick a general direction from your hypotheses above and steer towards that. \n",
    "\n",
    "I *also* know we haven't gotten far in terms of techniques so it may not be obvious what you even *could* eventually do. But trust your curiosity and instincts! It's ok if it's not something you ultimately end up doing, or even is feasible long term. You can also be quite general, like \"understand trends in X over time\" and leave it more or less as that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides stopwords in nltk, I also add several words commonly seen in this kind of blog: \\\"towardsdatascience\\\", the column name and \\\".com\\\" that authors tend to write their contact at the very end.\n",
    "\n",
    "After removing all stopwords, the word count of the post decreases over 40% to 603, while originally it\\' 1165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Put the NLTK stop words back in, and try removing stop words from a *different* list. It doesn't matter what list it is as long as it's not from NLTK or your own brain (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Stopwords: 1038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " 'attracts',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'people',\n",
       " 'from',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'professions',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'career',\n",
       " 'change',\n",
       " 'with',\n",
       " 'the',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'becoming',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'despite',\n",
       " 'the',\n",
       " 'high',\n",
       " 'demand',\n",
       " 'for',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'challenging',\n",
       " 'task',\n",
       " 'to',\n",
       " 'find',\n",
       " 'first',\n",
       " 'job',\n",
       " 'unless',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'prior',\n",
       " 'job',\n",
       " 'experience',\n",
       " 'interviews',\n",
       " 'show',\n",
       " 'skills',\n",
       " 'and',\n",
       " 'impress',\n",
       " 'potential',\n",
       " 'employer',\n",
       " 'data',\n",
       " 'science',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'which',\n",
       " 'covers',\n",
       " 'a',\n",
       " 'broad',\n",
       " 'range',\n",
       " 'of',\n",
       " 'topics',\n",
       " 'and',\n",
       " 'concepts',\n",
       " 'thus',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'questions',\n",
       " 'that',\n",
       " 'might',\n",
       " 'be',\n",
       " 'asked',\n",
       " 'at',\n",
       " 'an',\n",
       " 'interview',\n",
       " 'very',\n",
       " 'high',\n",
       " 'however',\n",
       " 'there',\n",
       " 'some',\n",
       " 'questions',\n",
       " 'about',\n",
       " 'the',\n",
       " 'fundamentals',\n",
       " 'in',\n",
       " 'data',\n",
       " 'science',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'these',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'do',\n",
       " 'not',\n",
       " 'want',\n",
       " 'to',\n",
       " 'miss',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'go',\n",
       " 'over',\n",
       " '10',\n",
       " 'questions',\n",
       " 'that',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'asked',\n",
       " 'at',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'interview',\n",
       " 'the',\n",
       " 'questions',\n",
       " 'grouped',\n",
       " 'into',\n",
       " '3',\n",
       " 'main',\n",
       " 'categories',\n",
       " 'which',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'python',\n",
       " 'and',\n",
       " 'sql',\n",
       " 'i',\n",
       " 'try',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'brief',\n",
       " 'answer',\n",
       " 'for',\n",
       " 'each',\n",
       " 'question',\n",
       " 'however',\n",
       " 'i',\n",
       " 'suggest',\n",
       " 'reading',\n",
       " 'or',\n",
       " 'studying',\n",
       " 'each',\n",
       " 'one',\n",
       " 'in',\n",
       " 'more',\n",
       " 'detail',\n",
       " 'afterwards',\n",
       " 'overfitting',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'occurs',\n",
       " 'model',\n",
       " 'not',\n",
       " 'generalized',\n",
       " 'well',\n",
       " 'the',\n",
       " 'model',\n",
       " 'too',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'the',\n",
       " 'training',\n",
       " 'set',\n",
       " 'captures',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'detail',\n",
       " 'or',\n",
       " 'even',\n",
       " 'noise',\n",
       " 'in',\n",
       " 'the',\n",
       " 'training',\n",
       " 'set',\n",
       " 'thus',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'the',\n",
       " 'general',\n",
       " 'trend',\n",
       " 'or',\n",
       " 'the',\n",
       " 'relationships',\n",
       " 'in',\n",
       " 'the',\n",
       " 'data',\n",
       " 'if',\n",
       " 'a',\n",
       " 'model',\n",
       " 'too',\n",
       " 'complex',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'data',\n",
       " 'probably',\n",
       " 'be',\n",
       " 'overfitting',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'indicator',\n",
       " 'of',\n",
       " 'overfitting',\n",
       " 'the',\n",
       " 'high',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'the',\n",
       " 'accuracy',\n",
       " 'of',\n",
       " 'training',\n",
       " 'and',\n",
       " 'test',\n",
       " 'sets',\n",
       " 'overfit',\n",
       " 'models',\n",
       " 'usually',\n",
       " 'very',\n",
       " 'high',\n",
       " 'accuracy',\n",
       " 'on',\n",
       " 'the',\n",
       " 'training',\n",
       " 'set',\n",
       " 'but',\n",
       " 'the',\n",
       " 'test',\n",
       " 'accuracy',\n",
       " 'usually',\n",
       " 'unpredictable',\n",
       " 'and',\n",
       " 'much',\n",
       " 'lower',\n",
       " 'than',\n",
       " 'the',\n",
       " 'training',\n",
       " 'accuracy',\n",
       " 'we',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'by',\n",
       " 'making',\n",
       " 'the',\n",
       " 'model',\n",
       " 'more',\n",
       " 'generalized',\n",
       " 'which',\n",
       " 'means',\n",
       " 'be',\n",
       " 'more',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'the',\n",
       " 'general',\n",
       " 'trend',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'specific',\n",
       " 'details',\n",
       " 'if',\n",
       " 'possible',\n",
       " 'collecting',\n",
       " 'more',\n",
       " 'data',\n",
       " 'an',\n",
       " 'efficient',\n",
       " 'way',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'be',\n",
       " 'giving',\n",
       " 'more',\n",
       " 'juice',\n",
       " 'to',\n",
       " 'the',\n",
       " 'model',\n",
       " 'so',\n",
       " 'more',\n",
       " 'material',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data',\n",
       " 'always',\n",
       " 'valuable',\n",
       " 'especially',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'another',\n",
       " 'method',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'model',\n",
       " 'if',\n",
       " 'a',\n",
       " 'model',\n",
       " 'too',\n",
       " 'complex',\n",
       " 'for',\n",
       " 'a',\n",
       " 'given',\n",
       " 'task',\n",
       " 'likely',\n",
       " 'result',\n",
       " 'in',\n",
       " 'overfitting',\n",
       " 'in',\n",
       " 'such',\n",
       " 'cases',\n",
       " 'we',\n",
       " 'look',\n",
       " 'for',\n",
       " 'simpler',\n",
       " 'models',\n",
       " 'we',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'the',\n",
       " 'main',\n",
       " 'reason',\n",
       " 'for',\n",
       " 'overfitting',\n",
       " 'a',\n",
       " 'model',\n",
       " 'being',\n",
       " 'more',\n",
       " 'complex',\n",
       " 'than',\n",
       " 'necessary',\n",
       " 'regularization',\n",
       " 'a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'reducing',\n",
       " 'the',\n",
       " 'model',\n",
       " 'complexity',\n",
       " 'does',\n",
       " 'so',\n",
       " 'by',\n",
       " 'penalizing',\n",
       " 'higher',\n",
       " 'terms',\n",
       " 'in',\n",
       " 'the',\n",
       " 'model',\n",
       " 'with',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'regularization',\n",
       " 'term',\n",
       " 'the',\n",
       " 'model',\n",
       " 'tries',\n",
       " 'to',\n",
       " 'minimize',\n",
       " 'both',\n",
       " 'loss',\n",
       " 'and',\n",
       " 'complexity',\n",
       " 'two',\n",
       " 'main',\n",
       " 'types',\n",
       " 'of',\n",
       " 'regularization',\n",
       " 'l1',\n",
       " 'and',\n",
       " 'l2',\n",
       " 'regularization',\n",
       " 'l1',\n",
       " 'regularization',\n",
       " 'subtracts',\n",
       " 'a',\n",
       " 'small',\n",
       " 'amount',\n",
       " 'from',\n",
       " 'the',\n",
       " 'weights',\n",
       " 'of',\n",
       " 'uninformative',\n",
       " 'features',\n",
       " 'at',\n",
       " 'each',\n",
       " 'iteration',\n",
       " 'thus',\n",
       " 'causes',\n",
       " 'these',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'eventually',\n",
       " 'become',\n",
       " 'zero',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'l2',\n",
       " 'regularization',\n",
       " 'removes',\n",
       " 'a',\n",
       " 'small',\n",
       " 'percentage',\n",
       " 'from',\n",
       " 'the',\n",
       " 'weights',\n",
       " 'at',\n",
       " 'each',\n",
       " 'iteration',\n",
       " 'these',\n",
       " 'weights',\n",
       " 'get',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'zero',\n",
       " 'but',\n",
       " 'never',\n",
       " 'actually',\n",
       " 'become',\n",
       " '0',\n",
       " 'both',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'tasks',\n",
       " 'classification',\n",
       " 'a',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'task',\n",
       " 'so',\n",
       " 'we',\n",
       " 'labelled',\n",
       " 'observations',\n",
       " '(',\n",
       " 'i',\n",
       " 'e',\n",
       " 'data',\n",
       " 'points',\n",
       " ')',\n",
       " 'we',\n",
       " 'train',\n",
       " 'a',\n",
       " 'model',\n",
       " 'with',\n",
       " 'labelled',\n",
       " 'data',\n",
       " 'and',\n",
       " 'expect',\n",
       " 'to',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'labels',\n",
       " 'of',\n",
       " 'new',\n",
       " 'data',\n",
       " 'for',\n",
       " 'instance',\n",
       " 'spam',\n",
       " 'email',\n",
       " 'detection',\n",
       " 'a',\n",
       " 'classification',\n",
       " 'task',\n",
       " 'we',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'model',\n",
       " 'with',\n",
       " 'several',\n",
       " 'emails',\n",
       " 'marked',\n",
       " 'as',\n",
       " 'spam',\n",
       " 'or',\n",
       " 'not',\n",
       " 'spam',\n",
       " 'after',\n",
       " 'the',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'with',\n",
       " 'those',\n",
       " 'emails',\n",
       " 'evaluate',\n",
       " 'the',\n",
       " 'new',\n",
       " 'emails',\n",
       " 'appropriately',\n",
       " 'clustering',\n",
       " 'an',\n",
       " 'unsupervised',\n",
       " 'learning',\n",
       " 'task',\n",
       " 'so',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'do',\n",
       " 'not',\n",
       " 'any',\n",
       " 'labels',\n",
       " 'the',\n",
       " 'model',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'evaluate',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'and',\n",
       " 'group',\n",
       " 'into',\n",
       " 'clusters',\n",
       " 'similar',\n",
       " 'observations',\n",
       " 'placed',\n",
       " 'into',\n",
       " 'the',\n",
       " 'same',\n",
       " 'cluster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'optimal',\n",
       " 'case',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'cluster',\n",
       " 'as',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'other',\n",
       " 'as',\n",
       " 'possible',\n",
       " 'and',\n",
       " 'the',\n",
       " 'different',\n",
       " 'clusters',\n",
       " 'as',\n",
       " 'far',\n",
       " 'apart',\n",
       " 'as',\n",
       " 'possible',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'a',\n",
       " 'clustering',\n",
       " 'task',\n",
       " 'be',\n",
       " 'grouping',\n",
       " 'customers',\n",
       " 'based',\n",
       " 'on',\n",
       " 'their',\n",
       " 'shopping',\n",
       " 'behavior',\n",
       " 'the',\n",
       " 'built-in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'of',\n",
       " 'crucial',\n",
       " 'importance',\n",
       " 'thus',\n",
       " 'be',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'and',\n",
       " 'to',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'list',\n",
       " 'dictionary',\n",
       " 'set',\n",
       " 'and',\n",
       " 'tuple',\n",
       " '4',\n",
       " 'main',\n",
       " 'built-in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'in',\n",
       " 'python',\n",
       " 'the',\n",
       " 'main',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'lists',\n",
       " 'and',\n",
       " 'tuples',\n",
       " 'mutability',\n",
       " 'lists',\n",
       " 'mutable',\n",
       " 'so',\n",
       " 'we',\n",
       " 'manipulate',\n",
       " 'by',\n",
       " 'adding',\n",
       " 'or',\n",
       " 'removing',\n",
       " 'items',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'tuples',\n",
       " 'immutable',\n",
       " 'although',\n",
       " 'we',\n",
       " 'access',\n",
       " 'each',\n",
       " 'element',\n",
       " 'in',\n",
       " 'a',\n",
       " 'tuple',\n",
       " 'we',\n",
       " 'not',\n",
       " 'modify',\n",
       " 'its',\n",
       " 'content',\n",
       " 'one',\n",
       " 'important',\n",
       " 'point',\n",
       " 'to',\n",
       " 'mention',\n",
       " 'here',\n",
       " 'that',\n",
       " 'although',\n",
       " 'tuples',\n",
       " 'immutable',\n",
       " 'contain',\n",
       " 'mutable',\n",
       " 'elements',\n",
       " 'such',\n",
       " 'as',\n",
       " 'lists',\n",
       " 'or',\n",
       " 'sets',\n",
       " 'let',\n",
       " '’',\n",
       " 's',\n",
       " 'do',\n",
       " 'an',\n",
       " 'example',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'the',\n",
       " 'main',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'lists',\n",
       " 'and',\n",
       " 'sets',\n",
       " 'as',\n",
       " 'we',\n",
       " 'notice',\n",
       " 'in',\n",
       " 'the',\n",
       " 'resulting',\n",
       " 'objects',\n",
       " 'the',\n",
       " 'list',\n",
       " 'contains',\n",
       " 'all',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'in',\n",
       " 'the',\n",
       " 'string',\n",
       " 'whereas',\n",
       " 'the',\n",
       " 'set',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'unique',\n",
       " 'values',\n",
       " 'another',\n",
       " 'difference',\n",
       " 'that',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'in',\n",
       " 'the',\n",
       " 'list',\n",
       " 'ordered',\n",
       " 'based',\n",
       " 'on',\n",
       " 'their',\n",
       " 'location',\n",
       " 'in',\n",
       " 'the',\n",
       " 'string',\n",
       " 'however',\n",
       " 'there',\n",
       " 'no',\n",
       " 'order',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " 'here',\n",
       " 'a',\n",
       " 'table',\n",
       " 'that',\n",
       " 'summarizes',\n",
       " 'the',\n",
       " 'main',\n",
       " 'characteristics',\n",
       " 'of',\n",
       " 'lists',\n",
       " 'tuples',\n",
       " 'and',\n",
       " 'sets',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'in',\n",
       " 'python',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'key-value',\n",
       " 'pairs',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'a',\n",
       " 'list',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sense',\n",
       " 'that',\n",
       " 'each',\n",
       " 'item',\n",
       " 'in',\n",
       " 'a',\n",
       " 'list',\n",
       " 'has',\n",
       " 'an',\n",
       " 'associated',\n",
       " 'index',\n",
       " 'starting',\n",
       " 'from',\n",
       " '0',\n",
       " 'in',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'we',\n",
       " 'keys',\n",
       " 'as',\n",
       " 'the',\n",
       " 'index',\n",
       " 'thus',\n",
       " 'we',\n",
       " 'access',\n",
       " 'a',\n",
       " 'value',\n",
       " 'by',\n",
       " 'using',\n",
       " 'its',\n",
       " 'key',\n",
       " 'the',\n",
       " 'keys',\n",
       " 'in',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'unique',\n",
       " 'which',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'because',\n",
       " 'act',\n",
       " 'like',\n",
       " 'an',\n",
       " 'address',\n",
       " 'for',\n",
       " 'the',\n",
       " 'values',\n",
       " 'sql',\n",
       " 'an',\n",
       " 'extremely',\n",
       " 'important',\n",
       " 'skill',\n",
       " 'for',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'there',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'companies',\n",
       " 'that',\n",
       " 'store',\n",
       " 'their',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'relational',\n",
       " 'database',\n",
       " 'sql',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'relational',\n",
       " 'databases',\n",
       " 'probably',\n",
       " 'be',\n",
       " 'asked',\n",
       " 'a',\n",
       " 'question',\n",
       " 'that',\n",
       " 'involves',\n",
       " 'writing',\n",
       " 'a',\n",
       " 'query',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'task',\n",
       " 'might',\n",
       " 'also',\n",
       " 'be',\n",
       " 'asked',\n",
       " 'a',\n",
       " 'question',\n",
       " 'about',\n",
       " 'general',\n",
       " 'database',\n",
       " 'knowledge',\n",
       " 'consider',\n",
       " 'we',\n",
       " 'a',\n",
       " 'sales',\n",
       " 'table',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'daily',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'of',\n",
       " 'products',\n",
       " 'find',\n",
       " 'the',\n",
       " 'top',\n",
       " '5',\n",
       " 'weeks',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'total',\n",
       " 'weekly',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'we',\n",
       " 'first',\n",
       " 'extract',\n",
       " 'the',\n",
       " 'year',\n",
       " 'and',\n",
       " 'week',\n",
       " 'information',\n",
       " 'from',\n",
       " 'the',\n",
       " 'date',\n",
       " 'column',\n",
       " 'and',\n",
       " 'then',\n",
       " 'use',\n",
       " 'in',\n",
       " 'the',\n",
       " 'aggregation',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'function',\n",
       " 'used',\n",
       " 'to',\n",
       " 'calculate',\n",
       " 'the',\n",
       " 'total',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'sales',\n",
       " 'table',\n",
       " 'find',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'unique',\n",
       " 'items',\n",
       " 'that',\n",
       " 'sold',\n",
       " 'each',\n",
       " 'month',\n",
       " 'these',\n",
       " 'terms',\n",
       " 'related',\n",
       " 'to',\n",
       " 'database',\n",
       " 'schema',\n",
       " 'design',\n",
       " 'normalization',\n",
       " 'and',\n",
       " 'denormalization',\n",
       " 'aim',\n",
       " 'to',\n",
       " 'optimize',\n",
       " 'different',\n",
       " 'metrics',\n",
       " 'the',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'normalization',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'data',\n",
       " 'redundancy',\n",
       " 'and',\n",
       " 'inconsistency',\n",
       " 'by',\n",
       " 'increasing',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'tables',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'denormalization',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'speed',\n",
       " 'up',\n",
       " 'the',\n",
       " 'query',\n",
       " 'execution',\n",
       " 'denormalization',\n",
       " 'decreases',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'tables',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'adds',\n",
       " 'some',\n",
       " 'redundancy',\n",
       " 'a',\n",
       " 'challenging',\n",
       " 'task',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'requires',\n",
       " 'time',\n",
       " 'effort',\n",
       " 'and',\n",
       " 'dedication',\n",
       " 'without',\n",
       " 'prior',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_custom = [\"towardsdatascience\",\".com\",\"is\",\"are\",\"will\",\"would\",\"wouldn't\",\"can\",\"could\",\"couldn't\",\"shall\",\"should\", \"shouldn't\",\"have\",\"having\",\"going\",\"aren't\",\"am\",\"isn't\",\"it\",\"they\",\"he\",\"she\",\"him\",\"her\",\"them\",\"you\",\"your\",\"what\",\"when\",\"who\",\"where\",\"how\"]\n",
    "tokens_no_sw_custom = []\n",
    "for token in tokens:\n",
    "    if token not in stopwords_custom:\n",
    "        tokens_no_sw_custom.append(token)\n",
    "print(f'Tokens without Stopwords: {len(tokens_no_sw_custom)}')\n",
    "tokens_no_sw_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) What stop word list did you use, and why did you choose it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"towardsdatascience, .com, is, are, will, would, wouldn't, can, could, couldn't, shall, should, shouldn't, have, having, going, aren't, am, isn't, it, they, he, she, him, her, them, you, your, what, when, who, where, how\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = \", \".join(stopwords_custom)\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customized word list is listed above. They are mostly verbs or pronouns or others that have no concrete meaning but make the sentence grammarly correct and understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) How do you think it did? Do you think it's more useful than the NLTK list, or not? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not perform as well as NLTK list does, because the customized list is a quite short one that I comed up with myself.\n",
    "\n",
    "A good strategy for latter use is to start from the NLTK stopwords list, and slightly modify the list according to the task needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) And now the moment you've all been waiting for -- it's time to think about some potentially useful stop words to explore dropping that might be unique to your text. First, what might some of these stop words be? Feel free to write some code below to work out what might be helpful (no need if that's not necessary). (Note: your final decision might be to remove no stop words, not even your own, but for now you must choose at least one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('.com')\n",
    "stopwords.append('towardsdatascience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) Go ahead and drop your unique stop words! (You may do so while retaining previous dropped stop words, or not, depending on what is more useful for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Stopwords: 603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['popularity',\n",
       " 'data',\n",
       " 'science',\n",
       " 'attracts',\n",
       " 'lot',\n",
       " 'people',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'professions',\n",
       " 'make',\n",
       " 'career',\n",
       " 'change',\n",
       " 'goal',\n",
       " 'becoming',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'despite',\n",
       " 'high',\n",
       " 'demand',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'highly',\n",
       " 'challenging',\n",
       " 'task',\n",
       " 'find',\n",
       " 'first',\n",
       " 'job',\n",
       " 'unless',\n",
       " 'solid',\n",
       " 'prior',\n",
       " 'job',\n",
       " 'experience',\n",
       " 'interviews',\n",
       " 'show',\n",
       " 'skills',\n",
       " 'impress',\n",
       " 'potential',\n",
       " 'employer',\n",
       " 'data',\n",
       " 'science',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'covers',\n",
       " 'broad',\n",
       " 'range',\n",
       " 'topics',\n",
       " 'concepts',\n",
       " 'thus',\n",
       " 'number',\n",
       " 'questions',\n",
       " 'might',\n",
       " 'asked',\n",
       " 'interview',\n",
       " 'high',\n",
       " 'however',\n",
       " 'questions',\n",
       " 'fundamentals',\n",
       " 'data',\n",
       " 'science',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ones',\n",
       " 'want',\n",
       " 'miss',\n",
       " 'article',\n",
       " 'go',\n",
       " '10',\n",
       " 'questions',\n",
       " 'likely',\n",
       " 'asked',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'interview',\n",
       " 'questions',\n",
       " 'grouped',\n",
       " '3',\n",
       " 'main',\n",
       " 'categories',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'python',\n",
       " 'sql',\n",
       " 'try',\n",
       " 'provide',\n",
       " 'brief',\n",
       " 'answer',\n",
       " 'question',\n",
       " 'however',\n",
       " 'suggest',\n",
       " 'reading',\n",
       " 'studying',\n",
       " 'one',\n",
       " 'detail',\n",
       " 'afterwards',\n",
       " 'overfitting',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'occurs',\n",
       " 'model',\n",
       " 'generalized',\n",
       " 'well',\n",
       " 'model',\n",
       " 'focused',\n",
       " 'training',\n",
       " 'set',\n",
       " 'captures',\n",
       " 'lot',\n",
       " 'detail',\n",
       " 'even',\n",
       " 'noise',\n",
       " 'training',\n",
       " 'set',\n",
       " 'thus',\n",
       " 'fails',\n",
       " 'capture',\n",
       " 'general',\n",
       " 'trend',\n",
       " 'relationships',\n",
       " 'data',\n",
       " 'model',\n",
       " 'complex',\n",
       " 'compared',\n",
       " 'data',\n",
       " 'probably',\n",
       " 'overfitting',\n",
       " 'strong',\n",
       " 'indicator',\n",
       " 'overfitting',\n",
       " 'high',\n",
       " 'difference',\n",
       " 'accuracy',\n",
       " 'training',\n",
       " 'test',\n",
       " 'sets',\n",
       " 'overfit',\n",
       " 'models',\n",
       " 'usually',\n",
       " 'high',\n",
       " 'accuracy',\n",
       " 'training',\n",
       " 'set',\n",
       " 'test',\n",
       " 'accuracy',\n",
       " 'usually',\n",
       " 'unpredictable',\n",
       " 'much',\n",
       " 'lower',\n",
       " 'training',\n",
       " 'accuracy',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'making',\n",
       " 'model',\n",
       " 'generalized',\n",
       " 'means',\n",
       " 'focused',\n",
       " 'general',\n",
       " 'trend',\n",
       " 'rather',\n",
       " 'specific',\n",
       " 'details',\n",
       " 'possible',\n",
       " 'collecting',\n",
       " 'data',\n",
       " 'efficient',\n",
       " 'way',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'giving',\n",
       " 'juice',\n",
       " 'model',\n",
       " 'material',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'always',\n",
       " 'valuable',\n",
       " 'especially',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'another',\n",
       " 'method',\n",
       " 'reduce',\n",
       " 'overfitting',\n",
       " 'reduce',\n",
       " 'complexity',\n",
       " 'model',\n",
       " 'model',\n",
       " 'complex',\n",
       " 'given',\n",
       " 'task',\n",
       " 'likely',\n",
       " 'result',\n",
       " 'overfitting',\n",
       " 'cases',\n",
       " 'look',\n",
       " 'simpler',\n",
       " 'models',\n",
       " 'mentioned',\n",
       " 'main',\n",
       " 'reason',\n",
       " 'overfitting',\n",
       " 'model',\n",
       " 'complex',\n",
       " 'necessary',\n",
       " 'regularization',\n",
       " 'method',\n",
       " 'reducing',\n",
       " 'model',\n",
       " 'complexity',\n",
       " 'penalizing',\n",
       " 'higher',\n",
       " 'terms',\n",
       " 'model',\n",
       " 'addition',\n",
       " 'regularization',\n",
       " 'term',\n",
       " 'model',\n",
       " 'tries',\n",
       " 'minimize',\n",
       " 'loss',\n",
       " 'complexity',\n",
       " 'two',\n",
       " 'main',\n",
       " 'types',\n",
       " 'regularization',\n",
       " 'l1',\n",
       " 'l2',\n",
       " 'regularization',\n",
       " 'l1',\n",
       " 'regularization',\n",
       " 'subtracts',\n",
       " 'small',\n",
       " 'amount',\n",
       " 'weights',\n",
       " 'uninformative',\n",
       " 'features',\n",
       " 'iteration',\n",
       " 'thus',\n",
       " 'causes',\n",
       " 'weights',\n",
       " 'eventually',\n",
       " 'become',\n",
       " 'zero',\n",
       " 'hand',\n",
       " 'l2',\n",
       " 'regularization',\n",
       " 'removes',\n",
       " 'small',\n",
       " 'percentage',\n",
       " 'weights',\n",
       " 'iteration',\n",
       " 'weights',\n",
       " 'get',\n",
       " 'closer',\n",
       " 'zero',\n",
       " 'never',\n",
       " 'actually',\n",
       " 'become',\n",
       " '0',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'tasks',\n",
       " 'classification',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'task',\n",
       " 'labelled',\n",
       " 'observations',\n",
       " '(',\n",
       " 'e',\n",
       " 'data',\n",
       " 'points',\n",
       " ')',\n",
       " 'train',\n",
       " 'model',\n",
       " 'labelled',\n",
       " 'data',\n",
       " 'expect',\n",
       " 'predict',\n",
       " 'labels',\n",
       " 'new',\n",
       " 'data',\n",
       " 'instance',\n",
       " 'spam',\n",
       " 'email',\n",
       " 'detection',\n",
       " 'classification',\n",
       " 'task',\n",
       " 'provide',\n",
       " 'model',\n",
       " 'several',\n",
       " 'emails',\n",
       " 'marked',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'emails',\n",
       " 'evaluate',\n",
       " 'new',\n",
       " 'emails',\n",
       " 'appropriately',\n",
       " 'clustering',\n",
       " 'unsupervised',\n",
       " 'learning',\n",
       " 'task',\n",
       " 'observations',\n",
       " 'labels',\n",
       " 'model',\n",
       " 'expected',\n",
       " 'evaluate',\n",
       " 'observations',\n",
       " 'group',\n",
       " 'clusters',\n",
       " 'similar',\n",
       " 'observations',\n",
       " 'placed',\n",
       " 'cluster',\n",
       " 'optimal',\n",
       " 'case',\n",
       " 'observations',\n",
       " 'cluster',\n",
       " 'close',\n",
       " 'possible',\n",
       " 'different',\n",
       " 'clusters',\n",
       " 'far',\n",
       " 'apart',\n",
       " 'possible',\n",
       " 'example',\n",
       " 'clustering',\n",
       " 'task',\n",
       " 'would',\n",
       " 'grouping',\n",
       " 'customers',\n",
       " 'based',\n",
       " 'shopping',\n",
       " 'behavior',\n",
       " 'built-in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'crucial',\n",
       " 'importance',\n",
       " 'thus',\n",
       " 'familiar',\n",
       " 'interact',\n",
       " 'list',\n",
       " 'dictionary',\n",
       " 'set',\n",
       " 'tuple',\n",
       " '4',\n",
       " 'main',\n",
       " 'built-in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'python',\n",
       " 'main',\n",
       " 'difference',\n",
       " 'lists',\n",
       " 'tuples',\n",
       " 'mutability',\n",
       " 'lists',\n",
       " 'mutable',\n",
       " 'manipulate',\n",
       " 'adding',\n",
       " 'removing',\n",
       " 'items',\n",
       " 'hand',\n",
       " 'tuples',\n",
       " 'immutable',\n",
       " 'although',\n",
       " 'access',\n",
       " 'element',\n",
       " 'tuple',\n",
       " 'modify',\n",
       " 'content',\n",
       " 'one',\n",
       " 'important',\n",
       " 'point',\n",
       " 'mention',\n",
       " 'although',\n",
       " 'tuples',\n",
       " 'immutable',\n",
       " 'contain',\n",
       " 'mutable',\n",
       " 'elements',\n",
       " 'lists',\n",
       " 'sets',\n",
       " 'let',\n",
       " '’',\n",
       " 'example',\n",
       " 'demonstrate',\n",
       " 'main',\n",
       " 'difference',\n",
       " 'lists',\n",
       " 'sets',\n",
       " 'notice',\n",
       " 'resulting',\n",
       " 'objects',\n",
       " 'list',\n",
       " 'contains',\n",
       " 'characters',\n",
       " 'string',\n",
       " 'whereas',\n",
       " 'set',\n",
       " 'contains',\n",
       " 'unique',\n",
       " 'values',\n",
       " 'another',\n",
       " 'difference',\n",
       " 'characters',\n",
       " 'list',\n",
       " 'ordered',\n",
       " 'based',\n",
       " 'location',\n",
       " 'string',\n",
       " 'however',\n",
       " 'order',\n",
       " 'associated',\n",
       " 'characters',\n",
       " 'set',\n",
       " 'table',\n",
       " 'summarizes',\n",
       " 'main',\n",
       " 'characteristics',\n",
       " 'lists',\n",
       " 'tuples',\n",
       " 'sets',\n",
       " 'dictionary',\n",
       " 'python',\n",
       " 'collection',\n",
       " 'key-value',\n",
       " 'pairs',\n",
       " 'similar',\n",
       " 'list',\n",
       " 'sense',\n",
       " 'item',\n",
       " 'list',\n",
       " 'associated',\n",
       " 'index',\n",
       " 'starting',\n",
       " '0',\n",
       " 'dictionary',\n",
       " 'keys',\n",
       " 'index',\n",
       " 'thus',\n",
       " 'access',\n",
       " 'value',\n",
       " 'using',\n",
       " 'key',\n",
       " 'keys',\n",
       " 'dictionary',\n",
       " 'unique',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'act',\n",
       " 'like',\n",
       " 'address',\n",
       " 'values',\n",
       " 'sql',\n",
       " 'extremely',\n",
       " 'important',\n",
       " 'skill',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'quite',\n",
       " 'number',\n",
       " 'companies',\n",
       " 'store',\n",
       " 'data',\n",
       " 'relational',\n",
       " 'database',\n",
       " 'sql',\n",
       " 'needed',\n",
       " 'interact',\n",
       " 'relational',\n",
       " 'databases',\n",
       " 'probably',\n",
       " 'asked',\n",
       " 'question',\n",
       " 'involves',\n",
       " 'writing',\n",
       " 'query',\n",
       " 'perform',\n",
       " 'specific',\n",
       " 'task',\n",
       " 'might',\n",
       " 'also',\n",
       " 'asked',\n",
       " 'question',\n",
       " 'general',\n",
       " 'database',\n",
       " 'knowledge',\n",
       " 'consider',\n",
       " 'sales',\n",
       " 'table',\n",
       " 'contains',\n",
       " 'daily',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'products',\n",
       " 'find',\n",
       " 'top',\n",
       " '5',\n",
       " 'weeks',\n",
       " 'terms',\n",
       " 'total',\n",
       " 'weekly',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'first',\n",
       " 'extract',\n",
       " 'year',\n",
       " 'week',\n",
       " 'information',\n",
       " 'date',\n",
       " 'column',\n",
       " 'use',\n",
       " 'aggregation',\n",
       " 'sum',\n",
       " 'function',\n",
       " 'used',\n",
       " 'calculate',\n",
       " 'total',\n",
       " 'sales',\n",
       " 'quantities',\n",
       " 'sales',\n",
       " 'table',\n",
       " 'find',\n",
       " 'number',\n",
       " 'unique',\n",
       " 'items',\n",
       " 'sold',\n",
       " 'month',\n",
       " 'terms',\n",
       " 'related',\n",
       " 'database',\n",
       " 'schema',\n",
       " 'design',\n",
       " 'normalization',\n",
       " 'denormalization',\n",
       " 'aim',\n",
       " 'optimize',\n",
       " 'different',\n",
       " 'metrics',\n",
       " 'goal',\n",
       " 'normalization',\n",
       " 'reduce',\n",
       " 'data',\n",
       " 'redundancy',\n",
       " 'inconsistency',\n",
       " 'increasing',\n",
       " 'number',\n",
       " 'tables',\n",
       " 'hand',\n",
       " 'denormalization',\n",
       " 'aims',\n",
       " 'speed',\n",
       " 'query',\n",
       " 'execution',\n",
       " 'denormalization',\n",
       " 'decreases',\n",
       " 'number',\n",
       " 'tables',\n",
       " 'time',\n",
       " 'adds',\n",
       " 'redundancy',\n",
       " 'challenging',\n",
       " 'task',\n",
       " 'become',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'requires',\n",
       " 'time',\n",
       " 'effort',\n",
       " 'dedication',\n",
       " 'without',\n",
       " 'prior',\n",
       " 'job',\n",
       " 'experience',\n",
       " 'process',\n",
       " 'gets',\n",
       " 'harder',\n",
       " 'interviews',\n",
       " 'important',\n",
       " 'demonstrate',\n",
       " 'skills',\n",
       " 'article',\n",
       " 'covered',\n",
       " '10',\n",
       " 'questions',\n",
       " 'likely',\n",
       " 'encounter',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'interview',\n",
       " 'thank',\n",
       " 'reading',\n",
       " 'please',\n",
       " 'let',\n",
       " 'know',\n",
       " 'feedback']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "tokens_no_sw_nltk_custom = []\n",
    "for token in tokens:\n",
    "    if token not in stopwords:\n",
    "        tokens_no_sw_nltk_custom.append(token)\n",
    "print(f'Tokens without Stopwords: {len(tokens_no_sw_nltk_custom)}')\n",
    "tokens_no_sw_nltk_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Having explored a few angles on stop words, what version do you think is best? (You could also choose, e.g., a subset of an existing list, or a subset + a few unique ones -- anything. If you do something outside of what we've already done anywhere in Question 4, just include your code below.) Briefly describe your stop word strategy and why you think it's the most useful. (As before, you may comment out any stop word code that you ultimately won't want to use going forward.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is the best, which I add some words to nltk stopwords list and form a customized stopword list.\n",
    "\n",
    "In such a way, we could still deal with most stopwords in English using nltk, but also include words that often appear in the towardsdatascience posts because those words couldn\\'t provide us much information when analyzing texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Go ahead and tokenize your text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tokenize the text before dealing with stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How many *tokens* are in your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 603 tokens in my corpus\n"
     ]
    }
   ],
   "source": [
    "print(f'there are {len(tokens_no_sw_nltk_custom)} tokens in my corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) How many *types* are in your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 345 types in my corpus\n"
     ]
    }
   ],
   "source": [
    "myset = set(tokens_no_sw_nltk_custom)\n",
    "print(f'There are {len(myset)} types in my corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) How many *terms* are in your corpus? How did you come to this number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 288 terms in my corpus.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#review = imdb.text[20]\n",
    "myset_stem_por = set()\n",
    "for token in tokens_no_sw_nltk_custom:\n",
    "    myset_stem_por.add(ps.stem(token))\n",
    "print(f'There are {len(myset_stem_por)} terms in my corpus.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stem all tokens to the root and count how many distinct tokens are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Is there anything else you'd like to share about your tokenizing experience? It's ok if the answer is \"no\", as tokenizing is (probably) the least controversial of all the steps. (Having said that, I'm sure the tokenization wars on Twitter will now blow up in my face!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope.\n",
    "\n",
    "One observation is that I also tried to deal with stopwords before tokenizing the text, where I used string.replace (stopword, ' ') method to drop stopwords. However it resulted in a mess because it\\'s possible for a stopword also exists as part another word, for example stopword \\\"he\\\" and word \\\"theater\\\". Thus I choosed to tokenize the text first and then drop stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) In lecture we talked about several strategies for POS tagging. Two of them were *lexical-based* and *rule-based*. Tag your corpus according to a *lexical-based* strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yuxia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('popularity', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('attracts', 'VBZ'),\n",
       " ('lot', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('wide', 'JJ'),\n",
       " ('range', 'VBP'),\n",
       " ('professions', 'NNS'),\n",
       " ('make', 'VBP'),\n",
       " ('career', 'NN'),\n",
       " ('change', 'NN'),\n",
       " ('goal', 'NN'),\n",
       " ('becoming', 'VBG'),\n",
       " ('data', 'NNS'),\n",
       " ('scientist', 'NN'),\n",
       " ('despite', 'IN'),\n",
       " ('high', 'JJ'),\n",
       " ('demand', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('scientists', 'NNS'),\n",
       " ('highly', 'RB'),\n",
       " ('challenging', 'VBG'),\n",
       " ('task', 'NN'),\n",
       " ('find', 'VBP'),\n",
       " ('first', 'JJ'),\n",
       " ('job', 'NN'),\n",
       " ('unless', 'IN'),\n",
       " ('solid', 'JJ'),\n",
       " ('prior', 'JJ'),\n",
       " ('job', 'NN'),\n",
       " ('experience', 'NN'),\n",
       " ('interviews', 'NNS'),\n",
       " ('show', 'VBP'),\n",
       " ('skills', 'VBZ'),\n",
       " ('impress', 'JJ'),\n",
       " ('potential', 'JJ'),\n",
       " ('employer', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('interdisciplinary', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('covers', 'VBZ'),\n",
       " ('broad', 'JJ'),\n",
       " ('range', 'NN'),\n",
       " ('topics', 'NNS'),\n",
       " ('concepts', 'NNS'),\n",
       " ('thus', 'RB'),\n",
       " ('number', 'NN'),\n",
       " ('questions', 'NNS'),\n",
       " ('might', 'MD'),\n",
       " ('asked', 'VBN'),\n",
       " ('interview', 'NN'),\n",
       " ('high', 'JJ'),\n",
       " ('however', 'RB'),\n",
       " ('questions', 'NNS'),\n",
       " ('fundamentals', 'NNS'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('ones', 'NNS'),\n",
       " ('want', 'VBP'),\n",
       " ('miss', 'JJ'),\n",
       " ('article', 'NN'),\n",
       " ('go', 'VBP'),\n",
       " ('10', 'CD'),\n",
       " ('questions', 'NNS'),\n",
       " ('likely', 'JJ'),\n",
       " ('asked', 'VBD'),\n",
       " ('data', 'NNS'),\n",
       " ('scientist', 'NN'),\n",
       " ('interview', 'NN'),\n",
       " ('questions', 'NNS'),\n",
       " ('grouped', 'VBD'),\n",
       " ('3', 'CD'),\n",
       " ('main', 'JJ'),\n",
       " ('categories', 'NNS'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('python', 'NN'),\n",
       " ('sql', 'NNS'),\n",
       " ('try', 'VBP'),\n",
       " ('provide', 'VBP'),\n",
       " ('brief', 'JJ'),\n",
       " ('answer', 'JJR'),\n",
       " ('question', 'NN'),\n",
       " ('however', 'RB'),\n",
       " ('suggest', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('studying', 'VBG'),\n",
       " ('one', 'CD'),\n",
       " ('detail', 'NN'),\n",
       " ('afterwards', 'NNS'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('occurs', 'VBZ'),\n",
       " ('model', 'NN'),\n",
       " ('generalized', 'VBN'),\n",
       " ('well', 'RB'),\n",
       " ('model', 'RB'),\n",
       " ('focused', 'JJ'),\n",
       " ('training', 'NN'),\n",
       " ('set', 'VBN'),\n",
       " ('captures', 'NNS'),\n",
       " ('lot', 'VBP'),\n",
       " ('detail', 'VB'),\n",
       " ('even', 'RB'),\n",
       " ('noise', 'JJ'),\n",
       " ('training', 'NN'),\n",
       " ('set', 'VBN'),\n",
       " ('thus', 'RB'),\n",
       " ('fails', 'JJ'),\n",
       " ('capture', 'NN'),\n",
       " ('general', 'JJ'),\n",
       " ('trend', 'NN'),\n",
       " ('relationships', 'NNS'),\n",
       " ('data', 'VBP'),\n",
       " ('model', 'NN'),\n",
       " ('complex', 'JJ'),\n",
       " ('compared', 'VBN'),\n",
       " ('data', 'NN'),\n",
       " ('probably', 'RB'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('strong', 'JJ'),\n",
       " ('indicator', 'NN'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('high', 'JJ'),\n",
       " ('difference', 'NN'),\n",
       " ('accuracy', 'NN'),\n",
       " ('training', 'VBG'),\n",
       " ('test', 'NN'),\n",
       " ('sets', 'NNS'),\n",
       " ('overfit', 'VBP'),\n",
       " ('models', 'NNS'),\n",
       " ('usually', 'RB'),\n",
       " ('high', 'JJ'),\n",
       " ('accuracy', 'NN'),\n",
       " ('training', 'NN'),\n",
       " ('set', 'VBN'),\n",
       " ('test', 'NN'),\n",
       " ('accuracy', 'NN'),\n",
       " ('usually', 'RB'),\n",
       " ('unpredictable', 'JJ'),\n",
       " ('much', 'JJ'),\n",
       " ('lower', 'JJR'),\n",
       " ('training', 'NN'),\n",
       " ('accuracy', 'NN'),\n",
       " ('reduce', 'VB'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('making', 'VBG'),\n",
       " ('model', 'NN'),\n",
       " ('generalized', 'VBN'),\n",
       " ('means', 'NNS'),\n",
       " ('focused', 'VBD'),\n",
       " ('general', 'JJ'),\n",
       " ('trend', 'NN'),\n",
       " ('rather', 'RB'),\n",
       " ('specific', 'JJ'),\n",
       " ('details', 'NNS'),\n",
       " ('possible', 'JJ'),\n",
       " ('collecting', 'VBG'),\n",
       " ('data', 'NNS'),\n",
       " ('efficient', 'JJ'),\n",
       " ('way', 'NN'),\n",
       " ('reduce', 'VB'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('giving', 'VBG'),\n",
       " ('juice', 'NN'),\n",
       " ('model', 'NN'),\n",
       " ('material', 'NN'),\n",
       " ('learn', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('always', 'RB'),\n",
       " ('valuable', 'JJ'),\n",
       " ('especially', 'RB'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('models', 'NNS'),\n",
       " ('another', 'DT'),\n",
       " ('method', 'NN'),\n",
       " ('reduce', 'VB'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('reduce', 'VB'),\n",
       " ('complexity', 'NN'),\n",
       " ('model', 'NN'),\n",
       " ('model', 'NN'),\n",
       " ('complex', 'JJ'),\n",
       " ('given', 'VBN'),\n",
       " ('task', 'NN'),\n",
       " ('likely', 'JJ'),\n",
       " ('result', 'NN'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('cases', 'NNS'),\n",
       " ('look', 'VBP'),\n",
       " ('simpler', 'JJ'),\n",
       " ('models', 'NNS'),\n",
       " ('mentioned', 'VBN'),\n",
       " ('main', 'JJ'),\n",
       " ('reason', 'NN'),\n",
       " ('overfitting', 'VBG'),\n",
       " ('model', 'NN'),\n",
       " ('complex', 'JJ'),\n",
       " ('necessary', 'JJ'),\n",
       " ('regularization', 'NN'),\n",
       " ('method', 'NN'),\n",
       " ('reducing', 'VBG'),\n",
       " ('model', 'NN'),\n",
       " ('complexity', 'NN'),\n",
       " ('penalizing', 'VBG'),\n",
       " ('higher', 'JJR'),\n",
       " ('terms', 'NNS'),\n",
       " ('model', 'VBP'),\n",
       " ('addition', 'NN'),\n",
       " ('regularization', 'NN'),\n",
       " ('term', 'NN'),\n",
       " ('model', 'NN'),\n",
       " ('tries', 'NNS'),\n",
       " ('minimize', 'VBP'),\n",
       " ('loss', 'NN'),\n",
       " ('complexity', 'NN'),\n",
       " ('two', 'CD'),\n",
       " ('main', 'JJ'),\n",
       " ('types', 'NNS'),\n",
       " ('regularization', 'NN'),\n",
       " ('l1', 'NN'),\n",
       " ('l2', 'JJ'),\n",
       " ('regularization', 'NN'),\n",
       " ('l1', 'NN'),\n",
       " ('regularization', 'NN'),\n",
       " ('subtracts', 'VBZ'),\n",
       " ('small', 'JJ'),\n",
       " ('amount', 'NN'),\n",
       " ('weights', 'NNS'),\n",
       " ('uninformative', 'JJ'),\n",
       " ('features', 'NNS'),\n",
       " ('iteration', 'VBP'),\n",
       " ('thus', 'RB'),\n",
       " ('causes', 'VBZ'),\n",
       " ('weights', 'NNS'),\n",
       " ('eventually', 'RB'),\n",
       " ('become', 'VBP'),\n",
       " ('zero', 'CD'),\n",
       " ('hand', 'NN'),\n",
       " ('l2', 'CC'),\n",
       " ('regularization', 'NN'),\n",
       " ('removes', 'VBZ'),\n",
       " ('small', 'JJ'),\n",
       " ('percentage', 'NN'),\n",
       " ('weights', 'NNS'),\n",
       " ('iteration', 'NN'),\n",
       " ('weights', 'NNS'),\n",
       " ('get', 'VBP'),\n",
       " ('closer', 'JJR'),\n",
       " ('zero', 'CD'),\n",
       " ('never', 'RB'),\n",
       " ('actually', 'RB'),\n",
       " ('become', 'VB'),\n",
       " ('0', 'CD'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('tasks', 'NNS'),\n",
       " ('classification', 'NN'),\n",
       " ('supervised', 'VBD'),\n",
       " ('learning', 'VBG'),\n",
       " ('task', 'NN'),\n",
       " ('labelled', 'VBD'),\n",
       " ('observations', 'NNS'),\n",
       " ('(', '('),\n",
       " ('e', 'VB'),\n",
       " ('data', 'NNS'),\n",
       " ('points', 'NNS'),\n",
       " (')', ')'),\n",
       " ('train', 'VBP'),\n",
       " ('model', 'NN'),\n",
       " ('labelled', 'VBN'),\n",
       " ('data', 'NNS'),\n",
       " ('expect', 'VBP'),\n",
       " ('predict', 'NN'),\n",
       " ('labels', 'NNS'),\n",
       " ('new', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('instance', 'NN'),\n",
       " ('spam', 'NN'),\n",
       " ('email', 'JJ'),\n",
       " ('detection', 'NN'),\n",
       " ('classification', 'NN'),\n",
       " ('task', 'NN'),\n",
       " ('provide', 'VBP'),\n",
       " ('model', 'NN'),\n",
       " ('several', 'JJ'),\n",
       " ('emails', 'NNS'),\n",
       " ('marked', 'VBD'),\n",
       " ('spam', 'JJ'),\n",
       " ('spam', 'JJ'),\n",
       " ('model', 'NN'),\n",
       " ('trained', 'VBD'),\n",
       " ('emails', 'NNS'),\n",
       " ('evaluate', 'VB'),\n",
       " ('new', 'JJ'),\n",
       " ('emails', 'NNS'),\n",
       " ('appropriately', 'RB'),\n",
       " ('clustering', 'VBG'),\n",
       " ('unsupervised', 'JJ'),\n",
       " ('learning', 'VBG'),\n",
       " ('task', 'NN'),\n",
       " ('observations', 'NNS'),\n",
       " ('labels', 'VBP'),\n",
       " ('model', 'NN'),\n",
       " ('expected', 'VBN'),\n",
       " ('evaluate', 'JJ'),\n",
       " ('observations', 'NNS'),\n",
       " ('group', 'NN'),\n",
       " ('clusters', 'NNS'),\n",
       " ('similar', 'JJ'),\n",
       " ('observations', 'NNS'),\n",
       " ('placed', 'VBD'),\n",
       " ('cluster', 'NN'),\n",
       " ('optimal', 'JJ'),\n",
       " ('case', 'NN'),\n",
       " ('observations', 'NNS'),\n",
       " ('cluster', 'VBP'),\n",
       " ('close', 'JJ'),\n",
       " ('possible', 'JJ'),\n",
       " ('different', 'JJ'),\n",
       " ('clusters', 'NNS'),\n",
       " ('far', 'RB'),\n",
       " ('apart', 'RB'),\n",
       " ('possible', 'JJ'),\n",
       " ('example', 'NN'),\n",
       " ('clustering', 'VBG'),\n",
       " ('task', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('grouping', 'VBG'),\n",
       " ('customers', 'NNS'),\n",
       " ('based', 'VBN'),\n",
       " ('shopping', 'VBG'),\n",
       " ('behavior', 'JJ'),\n",
       " ('built-in', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('structures', 'NNS'),\n",
       " ('crucial', 'JJ'),\n",
       " ('importance', 'NN'),\n",
       " ('thus', 'RB'),\n",
       " ('familiar', 'JJ'),\n",
       " ('interact', 'JJ'),\n",
       " ('list', 'NN'),\n",
       " ('dictionary', 'JJ'),\n",
       " ('set', 'VBD'),\n",
       " ('tuple', 'JJ'),\n",
       " ('4', 'CD'),\n",
       " ('main', 'JJ'),\n",
       " ('built-in', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('structures', 'NNS'),\n",
       " ('python', 'VBP'),\n",
       " ('main', 'JJ'),\n",
       " ('difference', 'NN'),\n",
       " ('lists', 'NNS'),\n",
       " ('tuples', 'VBP'),\n",
       " ('mutability', 'NN'),\n",
       " ('lists', 'NNS'),\n",
       " ('mutable', 'JJ'),\n",
       " ('manipulate', 'NN'),\n",
       " ('adding', 'VBG'),\n",
       " ('removing', 'VBG'),\n",
       " ('items', 'NNS'),\n",
       " ('hand', 'NN'),\n",
       " ('tuples', 'NNS'),\n",
       " ('immutable', 'JJ'),\n",
       " ('although', 'IN'),\n",
       " ('access', 'NN'),\n",
       " ('element', 'JJ'),\n",
       " ('tuple', 'NN'),\n",
       " ('modify', 'NN'),\n",
       " ('content', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('important', 'JJ'),\n",
       " ('point', 'NN'),\n",
       " ('mention', 'NN'),\n",
       " ('although', 'IN'),\n",
       " ('tuples', 'NNS'),\n",
       " ('immutable', 'JJ'),\n",
       " ('contain', 'NN'),\n",
       " ('mutable', 'JJ'),\n",
       " ('elements', 'NNS'),\n",
       " ('lists', 'VBZ'),\n",
       " ('sets', 'NNS'),\n",
       " ('let', 'VB'),\n",
       " ('’', 'NNP'),\n",
       " ('example', 'NN'),\n",
       " ('demonstrate', 'NN'),\n",
       " ('main', 'JJ'),\n",
       " ('difference', 'NN'),\n",
       " ('lists', 'NNS'),\n",
       " ('sets', 'NNS'),\n",
       " ('notice', 'RB'),\n",
       " ('resulting', 'VBG'),\n",
       " ('objects', 'NNS'),\n",
       " ('list', 'NN'),\n",
       " ('contains', 'VBZ'),\n",
       " ('characters', 'NNS'),\n",
       " ('string', 'VBG'),\n",
       " ('whereas', 'NNS'),\n",
       " ('set', 'VBP'),\n",
       " ('contains', 'NNS'),\n",
       " ('unique', 'JJ'),\n",
       " ('values', 'NNS'),\n",
       " ('another', 'DT'),\n",
       " ('difference', 'NN'),\n",
       " ('characters', 'NNS'),\n",
       " ('list', 'VBP'),\n",
       " ('ordered', 'VBN'),\n",
       " ('based', 'VBN'),\n",
       " ('location', 'NN'),\n",
       " ('string', 'VBG'),\n",
       " ('however', 'RB'),\n",
       " ('order', 'NN'),\n",
       " ('associated', 'JJ'),\n",
       " ('characters', 'NNS'),\n",
       " ('set', 'VBD'),\n",
       " ('table', 'JJ'),\n",
       " ('summarizes', 'NNS'),\n",
       " ('main', 'JJ'),\n",
       " ('characteristics', 'NNS'),\n",
       " ('lists', 'NNS'),\n",
       " ('tuples', 'NNS'),\n",
       " ('sets', 'NNS'),\n",
       " ('dictionary', 'JJ'),\n",
       " ('python', 'NN'),\n",
       " ('collection', 'NN'),\n",
       " ('key-value', 'JJ'),\n",
       " ('pairs', 'NNS'),\n",
       " ('similar', 'JJ'),\n",
       " ('list', 'NN'),\n",
       " ('sense', 'NN'),\n",
       " ('item', 'NN'),\n",
       " ('list', 'NN'),\n",
       " ('associated', 'VBN'),\n",
       " ('index', 'NN'),\n",
       " ('starting', 'VBG'),\n",
       " ('0', 'CD'),\n",
       " ('dictionary', 'JJ'),\n",
       " ('keys', 'NNS'),\n",
       " ('index', 'NN'),\n",
       " ('thus', 'RB'),\n",
       " ('access', 'NN'),\n",
       " ('value', 'NN'),\n",
       " ('using', 'VBG'),\n",
       " ('key', 'JJ'),\n",
       " ('keys', 'NNS'),\n",
       " ('dictionary', 'JJ'),\n",
       " ('unique', 'JJ'),\n",
       " ('makes', 'VBZ'),\n",
       " ('sense', 'NN'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('address', 'NN'),\n",
       " ('values', 'NNS'),\n",
       " ('sql', 'VBP'),\n",
       " ('extremely', 'RB'),\n",
       " ('important', 'JJ'),\n",
       " ('skill', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('scientists', 'NNS'),\n",
       " ('quite', 'RB'),\n",
       " ('number', 'NN'),\n",
       " ('companies', 'NNS'),\n",
       " ('store', 'VBP'),\n",
       " ('data', 'NNS'),\n",
       " ('relational', 'JJ'),\n",
       " ('database', 'NN'),\n",
       " ('sql', 'NN'),\n",
       " ('needed', 'VBN'),\n",
       " ('interact', 'JJ'),\n",
       " ('relational', 'JJ'),\n",
       " ('databases', 'NNS'),\n",
       " ('probably', 'RB'),\n",
       " ('asked', 'VBD'),\n",
       " ('question', 'NN'),\n",
       " ('involves', 'VBZ'),\n",
       " ('writing', 'VBG'),\n",
       " ('query', 'NN'),\n",
       " ('perform', 'NN'),\n",
       " ('specific', 'JJ'),\n",
       " ('task', 'NN'),\n",
       " ('might', 'MD'),\n",
       " ('also', 'RB'),\n",
       " ('asked', 'VBN'),\n",
       " ('question', 'NN'),\n",
       " ('general', 'JJ'),\n",
       " ('database', 'NN'),\n",
       " ('knowledge', 'NN'),\n",
       " ('consider', 'VB'),\n",
       " ('sales', 'NNS'),\n",
       " ('table', 'JJ'),\n",
       " ('contains', 'NNS'),\n",
       " ('daily', 'JJ'),\n",
       " ('sales', 'NNS'),\n",
       " ('quantities', 'NNS'),\n",
       " ('products', 'NNS'),\n",
       " ('find', 'VBP'),\n",
       " ('top', 'JJ'),\n",
       " ('5', 'CD'),\n",
       " ('weeks', 'NNS'),\n",
       " ('terms', 'NNS'),\n",
       " ('total', 'JJ'),\n",
       " ('weekly', 'JJ'),\n",
       " ('sales', 'NNS'),\n",
       " ('quantities', 'NNS'),\n",
       " ('first', 'RB'),\n",
       " ('extract', 'JJ'),\n",
       " ('year', 'NN'),\n",
       " ('week', 'NN'),\n",
       " ('information', 'NN'),\n",
       " ('date', 'NN'),\n",
       " ('column', 'NN'),\n",
       " ('use', 'NN'),\n",
       " ('aggregation', 'NN'),\n",
       " ('sum', 'NN'),\n",
       " ('function', 'NN'),\n",
       " ('used', 'VBN'),\n",
       " ('calculate', 'JJ'),\n",
       " ('total', 'JJ'),\n",
       " ('sales', 'NNS'),\n",
       " ('quantities', 'NNS'),\n",
       " ('sales', 'NNS'),\n",
       " ('table', 'JJ'),\n",
       " ('find', 'VBP'),\n",
       " ('number', 'NN'),\n",
       " ('unique', 'JJ'),\n",
       " ('items', 'NNS'),\n",
       " ('sold', 'VBN'),\n",
       " ('month', 'NN'),\n",
       " ('terms', 'NNS'),\n",
       " ('related', 'JJ'),\n",
       " ('database', 'NN'),\n",
       " ('schema', 'NN'),\n",
       " ('design', 'NN'),\n",
       " ('normalization', 'NN'),\n",
       " ('denormalization', 'NN'),\n",
       " ('aim', 'NN'),\n",
       " ('optimize', 'VB'),\n",
       " ('different', 'JJ'),\n",
       " ('metrics', 'NNS'),\n",
       " ('goal', 'NN'),\n",
       " ('normalization', 'NN'),\n",
       " ('reduce', 'VB'),\n",
       " ('data', 'NNS'),\n",
       " ('redundancy', 'NN'),\n",
       " ('inconsistency', 'NN'),\n",
       " ('increasing', 'VBG'),\n",
       " ('number', 'NN'),\n",
       " ('tables', 'NNS'),\n",
       " ('hand', 'NN'),\n",
       " ('denormalization', 'NN'),\n",
       " ('aims', 'NNS'),\n",
       " ('speed', 'VBP'),\n",
       " ('query', 'JJ'),\n",
       " ('execution', 'NN'),\n",
       " ('denormalization', 'NN'),\n",
       " ('decreases', 'VBZ'),\n",
       " ('number', 'NN'),\n",
       " ('tables', 'NNS'),\n",
       " ('time', 'NN'),\n",
       " ('adds', 'VBZ'),\n",
       " ('redundancy', 'NN'),\n",
       " ('challenging', 'VBG'),\n",
       " ('task', 'NN'),\n",
       " ('become', 'VBN'),\n",
       " ('data', 'NNS'),\n",
       " ('scientist', 'NN'),\n",
       " ('requires', 'VBZ'),\n",
       " ('time', 'NN'),\n",
       " ('effort', 'NN'),\n",
       " ('dedication', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('prior', 'JJ'),\n",
       " ('job', 'NN'),\n",
       " ('experience', 'NN'),\n",
       " ('process', 'NN'),\n",
       " ('gets', 'VBZ'),\n",
       " ('harder', 'JJR'),\n",
       " ('interviews', 'NNS'),\n",
       " ('important', 'JJ'),\n",
       " ('demonstrate', 'NN'),\n",
       " ('skills', 'NNS'),\n",
       " ('article', 'NN'),\n",
       " ('covered', 'VBD'),\n",
       " ('10', 'CD'),\n",
       " ('questions', 'NNS'),\n",
       " ('likely', 'JJ'),\n",
       " ('encounter', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('scientist', 'NN'),\n",
       " ('interview', 'NN'),\n",
       " ('thank', 'IN'),\n",
       " ('reading', 'VBG'),\n",
       " ('please', 'JJ'),\n",
       " ('let', 'VB'),\n",
       " ('know', 'VB'),\n",
       " ('feedback', 'VB')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "# lexical-based POS tagging\n",
    "nltk.pos_tag(tokens_no_sw_nltk_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Now, remove those tags and instead tag your corpus according to a *rule-based* strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity UNK\n",
      "data NNS\n",
      "science NN\n",
      "attracts VB\n",
      "lot UNK\n",
      "people UNK\n",
      "wide UNK\n",
      "range UNK\n",
      "professions UNK\n",
      "make UNK\n",
      "career UNK\n",
      "change UNK\n",
      "goal UNK\n",
      "becoming UNK\n",
      "data NNS\n",
      "scientist UNK\n",
      "despite UNK\n",
      "high UNK\n",
      "demand UNK\n",
      "data NNS\n",
      "scientists UNK\n",
      "highly UNK\n",
      "challenging UNK\n",
      "task UNK\n",
      "find UNK\n",
      "first UNK\n",
      "job UNK\n",
      "unless UNK\n",
      "solid UNK\n",
      "prior UNK\n",
      "job UNK\n",
      "experience UNK\n",
      "interviews UNK\n",
      "show UNK\n",
      "skills UNK\n",
      "impress UNK\n",
      "potential UNK\n",
      "employer UNK\n",
      "data NNS\n",
      "science NN\n",
      "interdisciplinary VB\n",
      "field UNK\n",
      "covers UNK\n",
      "broad UNK\n",
      "range UNK\n",
      "topics UNK\n",
      "concepts UNK\n",
      "thus UNK\n",
      "number UNK\n",
      "questions UNK\n",
      "might UNK\n",
      "asked UNK\n",
      "interview UNK\n",
      "high UNK\n",
      "however UNK\n",
      "questions UNK\n",
      "fundamentals UNK\n",
      "data NNS\n",
      "science NN\n",
      "machine VB\n",
      "learning UNK\n",
      "ones UNK\n",
      "want UNK\n",
      "miss UNK\n",
      "article UNK\n",
      "go UNK\n",
      "10 UNK\n",
      "questions UNK\n",
      "likely UNK\n",
      "asked UNK\n",
      "data NNS\n",
      "scientist UNK\n",
      "interview UNK\n",
      "questions UNK\n",
      "grouped UNK\n",
      "3 UNK\n",
      "main UNK\n",
      "categories UNK\n",
      "machine UNK\n",
      "learning UNK\n",
      "python UNK\n",
      "sql UNK\n",
      "try UNK\n",
      "provide UNK\n",
      "brief UNK\n",
      "answer UNK\n",
      "question UNK\n",
      "however UNK\n",
      "suggest UNK\n",
      "reading UNK\n",
      "studying UNK\n",
      "one UNK\n",
      "detail UNK\n",
      "afterwards UNK\n",
      "overfitting UNK\n",
      "machine UNK\n",
      "learning UNK\n",
      "occurs UNK\n",
      "model UNK\n",
      "generalized UNK\n",
      "well UNK\n",
      "model UNK\n",
      "focused UNK\n",
      "training UNK\n",
      "set UNK\n",
      "captures UNK\n",
      "lot UNK\n",
      "detail UNK\n",
      "even UNK\n",
      "noise UNK\n",
      "training UNK\n",
      "set UNK\n",
      "thus UNK\n",
      "fails UNK\n",
      "capture UNK\n",
      "general UNK\n",
      "trend UNK\n",
      "relationships UNK\n",
      "data NNS\n",
      "model UNK\n",
      "complex UNK\n",
      "compared UNK\n",
      "data NNS\n",
      "probably UNK\n",
      "overfitting UNK\n",
      "strong UNK\n",
      "indicator UNK\n",
      "overfitting UNK\n",
      "high UNK\n",
      "difference UNK\n",
      "accuracy UNK\n",
      "training UNK\n",
      "test UNK\n",
      "sets UNK\n",
      "overfit UNK\n",
      "models UNK\n",
      "usually UNK\n",
      "high UNK\n",
      "accuracy UNK\n",
      "training UNK\n",
      "set UNK\n",
      "test UNK\n",
      "accuracy UNK\n",
      "usually UNK\n",
      "unpredictable UNK\n",
      "much UNK\n",
      "lower UNK\n",
      "training UNK\n",
      "accuracy UNK\n",
      "reduce UNK\n",
      "overfitting UNK\n",
      "making UNK\n",
      "model UNK\n",
      "generalized UNK\n",
      "means UNK\n",
      "focused UNK\n",
      "general UNK\n",
      "trend UNK\n",
      "rather UNK\n",
      "specific UNK\n",
      "details UNK\n",
      "possible UNK\n",
      "collecting UNK\n",
      "data NNS\n",
      "efficient UNK\n",
      "way UNK\n",
      "reduce UNK\n",
      "overfitting UNK\n",
      "giving UNK\n",
      "juice UNK\n",
      "model UNK\n",
      "material UNK\n",
      "learn UNK\n",
      "data NNS\n",
      "always UNK\n",
      "valuable UNK\n",
      "especially UNK\n",
      "machine UNK\n",
      "learning UNK\n",
      "models UNK\n",
      "another UNK\n",
      "method UNK\n",
      "reduce UNK\n",
      "overfitting UNK\n",
      "reduce UNK\n",
      "complexity UNK\n",
      "model UNK\n",
      "model UNK\n",
      "complex UNK\n",
      "given UNK\n",
      "task UNK\n",
      "likely UNK\n",
      "result UNK\n",
      "overfitting UNK\n",
      "cases UNK\n",
      "look UNK\n",
      "simpler UNK\n",
      "models UNK\n",
      "mentioned UNK\n",
      "main UNK\n",
      "reason UNK\n",
      "overfitting UNK\n",
      "model UNK\n",
      "complex UNK\n",
      "necessary UNK\n",
      "regularization UNK\n",
      "method UNK\n",
      "reducing UNK\n",
      "model UNK\n",
      "complexity UNK\n",
      "penalizing UNK\n",
      "higher UNK\n",
      "terms UNK\n",
      "model UNK\n",
      "addition UNK\n",
      "regularization UNK\n",
      "term UNK\n",
      "model UNK\n",
      "tries UNK\n",
      "minimize UNK\n",
      "loss UNK\n",
      "complexity UNK\n",
      "two UNK\n",
      "main UNK\n",
      "types UNK\n",
      "regularization UNK\n",
      "l1 UNK\n",
      "l2 UNK\n",
      "regularization UNK\n",
      "l1 UNK\n",
      "regularization UNK\n",
      "subtracts UNK\n",
      "small UNK\n",
      "amount UNK\n",
      "weights UNK\n",
      "uninformative UNK\n",
      "features UNK\n",
      "iteration UNK\n",
      "thus UNK\n",
      "causes UNK\n",
      "weights UNK\n",
      "eventually UNK\n",
      "become UNK\n",
      "zero UNK\n",
      "hand UNK\n",
      "l2 UNK\n",
      "regularization UNK\n",
      "removes UNK\n",
      "small UNK\n",
      "percentage UNK\n",
      "weights UNK\n",
      "iteration UNK\n",
      "weights UNK\n",
      "get UNK\n",
      "closer UNK\n",
      "zero UNK\n",
      "never UNK\n",
      "actually UNK\n",
      "become UNK\n",
      "0 UNK\n",
      "machine UNK\n",
      "learning UNK\n",
      "tasks UNK\n",
      "classification UNK\n",
      "supervised UNK\n",
      "learning UNK\n",
      "task UNK\n",
      "labelled UNK\n",
      "observations UNK\n",
      "( UNK\n",
      "e UNK\n",
      "data NNS\n",
      "points UNK\n",
      ") UNK\n",
      "train UNK\n",
      "model UNK\n",
      "labelled UNK\n",
      "data NNS\n",
      "expect UNK\n",
      "predict UNK\n",
      "labels UNK\n",
      "new UNK\n",
      "data NNS\n",
      "instance UNK\n",
      "spam UNK\n",
      "email UNK\n",
      "detection UNK\n",
      "classification UNK\n",
      "task UNK\n",
      "provide UNK\n",
      "model UNK\n",
      "several UNK\n",
      "emails UNK\n",
      "marked UNK\n",
      "spam UNK\n",
      "spam UNK\n",
      "model UNK\n",
      "trained UNK\n",
      "emails UNK\n",
      "evaluate UNK\n",
      "new UNK\n",
      "emails UNK\n",
      "appropriately UNK\n",
      "clustering UNK\n",
      "unsupervised UNK\n",
      "learning UNK\n",
      "task UNK\n",
      "observations UNK\n",
      "labels UNK\n",
      "model UNK\n",
      "expected UNK\n",
      "evaluate UNK\n",
      "observations UNK\n",
      "group UNK\n",
      "clusters UNK\n",
      "similar UNK\n",
      "observations UNK\n",
      "placed UNK\n",
      "cluster UNK\n",
      "optimal UNK\n",
      "case UNK\n",
      "observations UNK\n",
      "cluster UNK\n",
      "close UNK\n",
      "possible UNK\n",
      "different UNK\n",
      "clusters UNK\n",
      "far UNK\n",
      "apart UNK\n",
      "possible UNK\n",
      "example UNK\n",
      "clustering UNK\n",
      "task UNK\n",
      "would UNK\n",
      "grouping UNK\n",
      "customers UNK\n",
      "based UNK\n",
      "shopping UNK\n",
      "behavior UNK\n",
      "built-in UNK\n",
      "data NNS\n",
      "structures UNK\n",
      "crucial UNK\n",
      "importance UNK\n",
      "thus UNK\n",
      "familiar UNK\n",
      "interact UNK\n",
      "list UNK\n",
      "dictionary UNK\n",
      "set UNK\n",
      "tuple UNK\n",
      "4 UNK\n",
      "main UNK\n",
      "built-in UNK\n",
      "data NNS\n",
      "structures UNK\n",
      "python UNK\n",
      "main UNK\n",
      "difference UNK\n",
      "lists UNK\n",
      "tuples UNK\n",
      "mutability UNK\n",
      "lists UNK\n",
      "mutable UNK\n",
      "manipulate UNK\n",
      "adding UNK\n",
      "removing UNK\n",
      "items UNK\n",
      "hand UNK\n",
      "tuples UNK\n",
      "immutable UNK\n",
      "although UNK\n",
      "access UNK\n",
      "element UNK\n",
      "tuple UNK\n",
      "modify UNK\n",
      "content UNK\n",
      "one UNK\n",
      "important UNK\n",
      "point UNK\n",
      "mention UNK\n",
      "although UNK\n",
      "tuples UNK\n",
      "immutable UNK\n",
      "contain UNK\n",
      "mutable UNK\n",
      "elements UNK\n",
      "lists UNK\n",
      "sets UNK\n",
      "let UNK\n",
      "’ UNK\n",
      "example UNK\n",
      "demonstrate UNK\n",
      "main UNK\n",
      "difference UNK\n",
      "lists UNK\n",
      "sets UNK\n",
      "notice UNK\n",
      "resulting UNK\n",
      "objects UNK\n",
      "list UNK\n",
      "contains UNK\n",
      "characters UNK\n",
      "string UNK\n",
      "whereas UNK\n",
      "set UNK\n",
      "contains UNK\n",
      "unique UNK\n",
      "values UNK\n",
      "another UNK\n",
      "difference UNK\n",
      "characters UNK\n",
      "list UNK\n",
      "ordered UNK\n",
      "based UNK\n",
      "location UNK\n",
      "string UNK\n",
      "however UNK\n",
      "order UNK\n",
      "associated UNK\n",
      "characters UNK\n",
      "set UNK\n",
      "table UNK\n",
      "summarizes UNK\n",
      "main UNK\n",
      "characteristics UNK\n",
      "lists UNK\n",
      "tuples UNK\n",
      "sets UNK\n",
      "dictionary UNK\n",
      "python UNK\n",
      "collection UNK\n",
      "key-value UNK\n",
      "pairs UNK\n",
      "similar UNK\n",
      "list UNK\n",
      "sense UNK\n",
      "item UNK\n",
      "list UNK\n",
      "associated UNK\n",
      "index UNK\n",
      "starting UNK\n",
      "0 UNK\n",
      "dictionary UNK\n",
      "keys UNK\n",
      "index UNK\n",
      "thus UNK\n",
      "access UNK\n",
      "value UNK\n",
      "using UNK\n",
      "key UNK\n",
      "keys UNK\n",
      "dictionary UNK\n",
      "unique UNK\n",
      "makes UNK\n",
      "sense UNK\n",
      "act UNK\n",
      "like UNK\n",
      "address UNK\n",
      "values UNK\n",
      "sql UNK\n",
      "extremely UNK\n",
      "important UNK\n",
      "skill UNK\n",
      "data NNS\n",
      "scientists UNK\n",
      "quite UNK\n",
      "number UNK\n",
      "companies UNK\n",
      "store UNK\n",
      "data NNS\n",
      "relational UNK\n",
      "database UNK\n",
      "sql UNK\n",
      "needed UNK\n",
      "interact UNK\n",
      "relational UNK\n",
      "databases UNK\n",
      "probably UNK\n",
      "asked UNK\n",
      "question UNK\n",
      "involves UNK\n",
      "writing UNK\n",
      "query UNK\n",
      "perform UNK\n",
      "specific UNK\n",
      "task UNK\n",
      "might UNK\n",
      "also UNK\n",
      "asked UNK\n",
      "question UNK\n",
      "general UNK\n",
      "database UNK\n",
      "knowledge UNK\n",
      "consider UNK\n",
      "sales UNK\n",
      "table UNK\n",
      "contains UNK\n",
      "daily UNK\n",
      "sales UNK\n",
      "quantities UNK\n",
      "products UNK\n",
      "find UNK\n",
      "top UNK\n",
      "5 UNK\n",
      "weeks UNK\n",
      "terms UNK\n",
      "total UNK\n",
      "weekly UNK\n",
      "sales UNK\n",
      "quantities UNK\n",
      "first UNK\n",
      "extract UNK\n",
      "year UNK\n",
      "week UNK\n",
      "information UNK\n",
      "date UNK\n",
      "column UNK\n",
      "use UNK\n",
      "aggregation UNK\n",
      "sum UNK\n",
      "function UNK\n",
      "used UNK\n",
      "calculate UNK\n",
      "total UNK\n",
      "sales UNK\n",
      "quantities UNK\n",
      "sales UNK\n",
      "table UNK\n",
      "find UNK\n",
      "number UNK\n",
      "unique UNK\n",
      "items UNK\n",
      "sold UNK\n",
      "month UNK\n",
      "terms UNK\n",
      "related UNK\n",
      "database UNK\n",
      "schema UNK\n",
      "design UNK\n",
      "normalization UNK\n",
      "denormalization UNK\n",
      "aim UNK\n",
      "optimize UNK\n",
      "different UNK\n",
      "metrics UNK\n",
      "goal UNK\n",
      "normalization UNK\n",
      "reduce UNK\n",
      "data NNS\n",
      "redundancy UNK\n",
      "inconsistency UNK\n",
      "increasing UNK\n",
      "number UNK\n",
      "tables UNK\n",
      "hand UNK\n",
      "denormalization UNK\n",
      "aims UNK\n",
      "speed UNK\n",
      "query UNK\n",
      "execution UNK\n",
      "denormalization UNK\n",
      "decreases UNK\n",
      "number UNK\n",
      "tables UNK\n",
      "time UNK\n",
      "adds UNK\n",
      "redundancy UNK\n",
      "challenging UNK\n",
      "task UNK\n",
      "become UNK\n",
      "data NNS\n",
      "scientist UNK\n",
      "requires UNK\n",
      "time UNK\n",
      "effort UNK\n",
      "dedication UNK\n",
      "without UNK\n",
      "prior UNK\n",
      "job UNK\n",
      "experience UNK\n",
      "process UNK\n",
      "gets UNK\n",
      "harder UNK\n",
      "interviews UNK\n",
      "important UNK\n",
      "demonstrate UNK\n",
      "skills UNK\n",
      "article UNK\n",
      "covered UNK\n",
      "10 UNK\n",
      "questions UNK\n",
      "likely UNK\n",
      "encounter UNK\n",
      "data NNS\n",
      "scientist UNK\n",
      "interview UNK\n",
      "thank UNK\n",
      "reading UNK\n",
      "please UNK\n",
      "let UNK\n",
      "know UNK\n",
      "feedback UNK\n"
     ]
    }
   ],
   "source": [
    "# rule-based tagging (if-then statements)\n",
    "\n",
    "tags = []\n",
    "i = 0\n",
    "for token in tokens_no_sw_nltk_custom:\n",
    "  tag = 'UNK'\n",
    "  if token == 'data':\n",
    "    tag = 'NNS'\n",
    "  if token == 'science':\n",
    "    if tags[i-1] == 'NNS':\n",
    "        tag = 'NN'\n",
    "  if len(tags) > 1:\n",
    "    if tags[i-1] == 'NN':\n",
    "      tag = 'VB'\n",
    "  tags.append(tag)\n",
    "  i+= 1\n",
    "\n",
    "for i in range(len(tags)):\n",
    "  print(tokens_no_sw_nltk_custom[i], tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Which strategy do you think is more useful for your analysis, and why? (As ever, you may comment out the one you don't choose.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer lexical-based tagging, because it is much more efficient in large text, especially when we don't have knowledge of what's the text looking like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Let's stem!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) In lecture we discussed two stemmers: Porter and Lancaster. Apply the Porter stemmer to your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popularity', 'popular']\n",
      "['data', 'data']\n",
      "['science', 'scienc']\n",
      "['attracts', 'attract']\n",
      "['lot', 'lot']\n",
      "['people', 'peopl']\n",
      "['wide', 'wide']\n",
      "['range', 'rang']\n",
      "['professions', 'profess']\n",
      "['make', 'make']\n",
      "['career', 'career']\n",
      "['change', 'chang']\n",
      "['goal', 'goal']\n",
      "['becoming', 'becom']\n",
      "['data', 'data']\n",
      "['scientist', 'scientist']\n",
      "['despite', 'despit']\n",
      "['high', 'high']\n",
      "['demand', 'demand']\n",
      "['data', 'data']\n",
      "['scientists', 'scientist']\n",
      "['highly', 'highli']\n",
      "['challenging', 'challeng']\n",
      "['task', 'task']\n",
      "['find', 'find']\n",
      "['first', 'first']\n",
      "['job', 'job']\n",
      "['unless', 'unless']\n",
      "['solid', 'solid']\n",
      "['prior', 'prior']\n",
      "['job', 'job']\n",
      "['experience', 'experi']\n",
      "['interviews', 'interview']\n",
      "['show', 'show']\n",
      "['skills', 'skill']\n",
      "['impress', 'impress']\n",
      "['potential', 'potenti']\n",
      "['employer', 'employ']\n",
      "['data', 'data']\n",
      "['science', 'scienc']\n",
      "['interdisciplinary', 'interdisciplinari']\n",
      "['field', 'field']\n",
      "['covers', 'cover']\n",
      "['broad', 'broad']\n",
      "['range', 'rang']\n",
      "['topics', 'topic']\n",
      "['concepts', 'concept']\n",
      "['thus', 'thu']\n",
      "['number', 'number']\n",
      "['questions', 'question']\n",
      "['might', 'might']\n",
      "['asked', 'ask']\n",
      "['interview', 'interview']\n",
      "['high', 'high']\n",
      "['however', 'howev']\n",
      "['questions', 'question']\n",
      "['fundamentals', 'fundament']\n",
      "['data', 'data']\n",
      "['science', 'scienc']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['ones', 'one']\n",
      "['want', 'want']\n",
      "['miss', 'miss']\n",
      "['article', 'articl']\n",
      "['go', 'go']\n",
      "['10', '10']\n",
      "['questions', 'question']\n",
      "['likely', 'like']\n",
      "['asked', 'ask']\n",
      "['data', 'data']\n",
      "['scientist', 'scientist']\n",
      "['interview', 'interview']\n",
      "['questions', 'question']\n",
      "['grouped', 'group']\n",
      "['3', '3']\n",
      "['main', 'main']\n",
      "['categories', 'categori']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['python', 'python']\n",
      "['sql', 'sql']\n",
      "['try', 'tri']\n",
      "['provide', 'provid']\n",
      "['brief', 'brief']\n",
      "['answer', 'answer']\n",
      "['question', 'question']\n",
      "['however', 'howev']\n",
      "['suggest', 'suggest']\n",
      "['reading', 'read']\n",
      "['studying', 'studi']\n",
      "['one', 'one']\n",
      "['detail', 'detail']\n",
      "['afterwards', 'afterward']\n",
      "['overfitting', 'overfit']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['occurs', 'occur']\n",
      "['model', 'model']\n",
      "['generalized', 'gener']\n",
      "['well', 'well']\n",
      "['model', 'model']\n",
      "['focused', 'focus']\n",
      "['training', 'train']\n",
      "['set', 'set']\n",
      "['captures', 'captur']\n",
      "['lot', 'lot']\n",
      "['detail', 'detail']\n",
      "['even', 'even']\n",
      "['noise', 'nois']\n",
      "['training', 'train']\n",
      "['set', 'set']\n",
      "['thus', 'thu']\n",
      "['fails', 'fail']\n",
      "['capture', 'captur']\n",
      "['general', 'gener']\n",
      "['trend', 'trend']\n",
      "['relationships', 'relationship']\n",
      "['data', 'data']\n",
      "['model', 'model']\n",
      "['complex', 'complex']\n",
      "['compared', 'compar']\n",
      "['data', 'data']\n",
      "['probably', 'probabl']\n",
      "['overfitting', 'overfit']\n",
      "['strong', 'strong']\n",
      "['indicator', 'indic']\n",
      "['overfitting', 'overfit']\n",
      "['high', 'high']\n",
      "['difference', 'differ']\n",
      "['accuracy', 'accuraci']\n",
      "['training', 'train']\n",
      "['test', 'test']\n",
      "['sets', 'set']\n",
      "['overfit', 'overfit']\n",
      "['models', 'model']\n",
      "['usually', 'usual']\n",
      "['high', 'high']\n",
      "['accuracy', 'accuraci']\n",
      "['training', 'train']\n",
      "['set', 'set']\n",
      "['test', 'test']\n",
      "['accuracy', 'accuraci']\n",
      "['usually', 'usual']\n",
      "['unpredictable', 'unpredict']\n",
      "['much', 'much']\n",
      "['lower', 'lower']\n",
      "['training', 'train']\n",
      "['accuracy', 'accuraci']\n",
      "['reduce', 'reduc']\n",
      "['overfitting', 'overfit']\n",
      "['making', 'make']\n",
      "['model', 'model']\n",
      "['generalized', 'gener']\n",
      "['means', 'mean']\n",
      "['focused', 'focus']\n",
      "['general', 'gener']\n",
      "['trend', 'trend']\n",
      "['rather', 'rather']\n",
      "['specific', 'specif']\n",
      "['details', 'detail']\n",
      "['possible', 'possibl']\n",
      "['collecting', 'collect']\n",
      "['data', 'data']\n",
      "['efficient', 'effici']\n",
      "['way', 'way']\n",
      "['reduce', 'reduc']\n",
      "['overfitting', 'overfit']\n",
      "['giving', 'give']\n",
      "['juice', 'juic']\n",
      "['model', 'model']\n",
      "['material', 'materi']\n",
      "['learn', 'learn']\n",
      "['data', 'data']\n",
      "['always', 'alway']\n",
      "['valuable', 'valuabl']\n",
      "['especially', 'especi']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['models', 'model']\n",
      "['another', 'anoth']\n",
      "['method', 'method']\n",
      "['reduce', 'reduc']\n",
      "['overfitting', 'overfit']\n",
      "['reduce', 'reduc']\n",
      "['complexity', 'complex']\n",
      "['model', 'model']\n",
      "['model', 'model']\n",
      "['complex', 'complex']\n",
      "['given', 'given']\n",
      "['task', 'task']\n",
      "['likely', 'like']\n",
      "['result', 'result']\n",
      "['overfitting', 'overfit']\n",
      "['cases', 'case']\n",
      "['look', 'look']\n",
      "['simpler', 'simpler']\n",
      "['models', 'model']\n",
      "['mentioned', 'mention']\n",
      "['main', 'main']\n",
      "['reason', 'reason']\n",
      "['overfitting', 'overfit']\n",
      "['model', 'model']\n",
      "['complex', 'complex']\n",
      "['necessary', 'necessari']\n",
      "['regularization', 'regular']\n",
      "['method', 'method']\n",
      "['reducing', 'reduc']\n",
      "['model', 'model']\n",
      "['complexity', 'complex']\n",
      "['penalizing', 'penal']\n",
      "['higher', 'higher']\n",
      "['terms', 'term']\n",
      "['model', 'model']\n",
      "['addition', 'addit']\n",
      "['regularization', 'regular']\n",
      "['term', 'term']\n",
      "['model', 'model']\n",
      "['tries', 'tri']\n",
      "['minimize', 'minim']\n",
      "['loss', 'loss']\n",
      "['complexity', 'complex']\n",
      "['two', 'two']\n",
      "['main', 'main']\n",
      "['types', 'type']\n",
      "['regularization', 'regular']\n",
      "['l1', 'l1']\n",
      "['l2', 'l2']\n",
      "['regularization', 'regular']\n",
      "['l1', 'l1']\n",
      "['regularization', 'regular']\n",
      "['subtracts', 'subtract']\n",
      "['small', 'small']\n",
      "['amount', 'amount']\n",
      "['weights', 'weight']\n",
      "['uninformative', 'uninform']\n",
      "['features', 'featur']\n",
      "['iteration', 'iter']\n",
      "['thus', 'thu']\n",
      "['causes', 'caus']\n",
      "['weights', 'weight']\n",
      "['eventually', 'eventu']\n",
      "['become', 'becom']\n",
      "['zero', 'zero']\n",
      "['hand', 'hand']\n",
      "['l2', 'l2']\n",
      "['regularization', 'regular']\n",
      "['removes', 'remov']\n",
      "['small', 'small']\n",
      "['percentage', 'percentag']\n",
      "['weights', 'weight']\n",
      "['iteration', 'iter']\n",
      "['weights', 'weight']\n",
      "['get', 'get']\n",
      "['closer', 'closer']\n",
      "['zero', 'zero']\n",
      "['never', 'never']\n",
      "['actually', 'actual']\n",
      "['become', 'becom']\n",
      "['0', '0']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['tasks', 'task']\n",
      "['classification', 'classif']\n",
      "['supervised', 'supervis']\n",
      "['learning', 'learn']\n",
      "['task', 'task']\n",
      "['labelled', 'label']\n",
      "['observations', 'observ']\n",
      "['(', '(']\n",
      "['e', 'e']\n",
      "['data', 'data']\n",
      "['points', 'point']\n",
      "[')', ')']\n",
      "['train', 'train']\n",
      "['model', 'model']\n",
      "['labelled', 'label']\n",
      "['data', 'data']\n",
      "['expect', 'expect']\n",
      "['predict', 'predict']\n",
      "['labels', 'label']\n",
      "['new', 'new']\n",
      "['data', 'data']\n",
      "['instance', 'instanc']\n",
      "['spam', 'spam']\n",
      "['email', 'email']\n",
      "['detection', 'detect']\n",
      "['classification', 'classif']\n",
      "['task', 'task']\n",
      "['provide', 'provid']\n",
      "['model', 'model']\n",
      "['several', 'sever']\n",
      "['emails', 'email']\n",
      "['marked', 'mark']\n",
      "['spam', 'spam']\n",
      "['spam', 'spam']\n",
      "['model', 'model']\n",
      "['trained', 'train']\n",
      "['emails', 'email']\n",
      "['evaluate', 'evalu']\n",
      "['new', 'new']\n",
      "['emails', 'email']\n",
      "['appropriately', 'appropri']\n",
      "['clustering', 'cluster']\n",
      "['unsupervised', 'unsupervis']\n",
      "['learning', 'learn']\n",
      "['task', 'task']\n",
      "['observations', 'observ']\n",
      "['labels', 'label']\n",
      "['model', 'model']\n",
      "['expected', 'expect']\n",
      "['evaluate', 'evalu']\n",
      "['observations', 'observ']\n",
      "['group', 'group']\n",
      "['clusters', 'cluster']\n",
      "['similar', 'similar']\n",
      "['observations', 'observ']\n",
      "['placed', 'place']\n",
      "['cluster', 'cluster']\n",
      "['optimal', 'optim']\n",
      "['case', 'case']\n",
      "['observations', 'observ']\n",
      "['cluster', 'cluster']\n",
      "['close', 'close']\n",
      "['possible', 'possibl']\n",
      "['different', 'differ']\n",
      "['clusters', 'cluster']\n",
      "['far', 'far']\n",
      "['apart', 'apart']\n",
      "['possible', 'possibl']\n",
      "['example', 'exampl']\n",
      "['clustering', 'cluster']\n",
      "['task', 'task']\n",
      "['would', 'would']\n",
      "['grouping', 'group']\n",
      "['customers', 'custom']\n",
      "['based', 'base']\n",
      "['shopping', 'shop']\n",
      "['behavior', 'behavior']\n",
      "['built-in', 'built-in']\n",
      "['data', 'data']\n",
      "['structures', 'structur']\n",
      "['crucial', 'crucial']\n",
      "['importance', 'import']\n",
      "['thus', 'thu']\n",
      "['familiar', 'familiar']\n",
      "['interact', 'interact']\n",
      "['list', 'list']\n",
      "['dictionary', 'dictionari']\n",
      "['set', 'set']\n",
      "['tuple', 'tupl']\n",
      "['4', '4']\n",
      "['main', 'main']\n",
      "['built-in', 'built-in']\n",
      "['data', 'data']\n",
      "['structures', 'structur']\n",
      "['python', 'python']\n",
      "['main', 'main']\n",
      "['difference', 'differ']\n",
      "['lists', 'list']\n",
      "['tuples', 'tupl']\n",
      "['mutability', 'mutabl']\n",
      "['lists', 'list']\n",
      "['mutable', 'mutabl']\n",
      "['manipulate', 'manipul']\n",
      "['adding', 'ad']\n",
      "['removing', 'remov']\n",
      "['items', 'item']\n",
      "['hand', 'hand']\n",
      "['tuples', 'tupl']\n",
      "['immutable', 'immut']\n",
      "['although', 'although']\n",
      "['access', 'access']\n",
      "['element', 'element']\n",
      "['tuple', 'tupl']\n",
      "['modify', 'modifi']\n",
      "['content', 'content']\n",
      "['one', 'one']\n",
      "['important', 'import']\n",
      "['point', 'point']\n",
      "['mention', 'mention']\n",
      "['although', 'although']\n",
      "['tuples', 'tupl']\n",
      "['immutable', 'immut']\n",
      "['contain', 'contain']\n",
      "['mutable', 'mutabl']\n",
      "['elements', 'element']\n",
      "['lists', 'list']\n",
      "['sets', 'set']\n",
      "['let', 'let']\n",
      "['’', '’']\n",
      "['example', 'exampl']\n",
      "['demonstrate', 'demonstr']\n",
      "['main', 'main']\n",
      "['difference', 'differ']\n",
      "['lists', 'list']\n",
      "['sets', 'set']\n",
      "['notice', 'notic']\n",
      "['resulting', 'result']\n",
      "['objects', 'object']\n",
      "['list', 'list']\n",
      "['contains', 'contain']\n",
      "['characters', 'charact']\n",
      "['string', 'string']\n",
      "['whereas', 'wherea']\n",
      "['set', 'set']\n",
      "['contains', 'contain']\n",
      "['unique', 'uniqu']\n",
      "['values', 'valu']\n",
      "['another', 'anoth']\n",
      "['difference', 'differ']\n",
      "['characters', 'charact']\n",
      "['list', 'list']\n",
      "['ordered', 'order']\n",
      "['based', 'base']\n",
      "['location', 'locat']\n",
      "['string', 'string']\n",
      "['however', 'howev']\n",
      "['order', 'order']\n",
      "['associated', 'associ']\n",
      "['characters', 'charact']\n",
      "['set', 'set']\n",
      "['table', 'tabl']\n",
      "['summarizes', 'summar']\n",
      "['main', 'main']\n",
      "['characteristics', 'characterist']\n",
      "['lists', 'list']\n",
      "['tuples', 'tupl']\n",
      "['sets', 'set']\n",
      "['dictionary', 'dictionari']\n",
      "['python', 'python']\n",
      "['collection', 'collect']\n",
      "['key-value', 'key-valu']\n",
      "['pairs', 'pair']\n",
      "['similar', 'similar']\n",
      "['list', 'list']\n",
      "['sense', 'sens']\n",
      "['item', 'item']\n",
      "['list', 'list']\n",
      "['associated', 'associ']\n",
      "['index', 'index']\n",
      "['starting', 'start']\n",
      "['0', '0']\n",
      "['dictionary', 'dictionari']\n",
      "['keys', 'key']\n",
      "['index', 'index']\n",
      "['thus', 'thu']\n",
      "['access', 'access']\n",
      "['value', 'valu']\n",
      "['using', 'use']\n",
      "['key', 'key']\n",
      "['keys', 'key']\n",
      "['dictionary', 'dictionari']\n",
      "['unique', 'uniqu']\n",
      "['makes', 'make']\n",
      "['sense', 'sens']\n",
      "['act', 'act']\n",
      "['like', 'like']\n",
      "['address', 'address']\n",
      "['values', 'valu']\n",
      "['sql', 'sql']\n",
      "['extremely', 'extrem']\n",
      "['important', 'import']\n",
      "['skill', 'skill']\n",
      "['data', 'data']\n",
      "['scientists', 'scientist']\n",
      "['quite', 'quit']\n",
      "['number', 'number']\n",
      "['companies', 'compani']\n",
      "['store', 'store']\n",
      "['data', 'data']\n",
      "['relational', 'relat']\n",
      "['database', 'databas']\n",
      "['sql', 'sql']\n",
      "['needed', 'need']\n",
      "['interact', 'interact']\n",
      "['relational', 'relat']\n",
      "['databases', 'databas']\n",
      "['probably', 'probabl']\n",
      "['asked', 'ask']\n",
      "['question', 'question']\n",
      "['involves', 'involv']\n",
      "['writing', 'write']\n",
      "['query', 'queri']\n",
      "['perform', 'perform']\n",
      "['specific', 'specif']\n",
      "['task', 'task']\n",
      "['might', 'might']\n",
      "['also', 'also']\n",
      "['asked', 'ask']\n",
      "['question', 'question']\n",
      "['general', 'gener']\n",
      "['database', 'databas']\n",
      "['knowledge', 'knowledg']\n",
      "['consider', 'consid']\n",
      "['sales', 'sale']\n",
      "['table', 'tabl']\n",
      "['contains', 'contain']\n",
      "['daily', 'daili']\n",
      "['sales', 'sale']\n",
      "['quantities', 'quantiti']\n",
      "['products', 'product']\n",
      "['find', 'find']\n",
      "['top', 'top']\n",
      "['5', '5']\n",
      "['weeks', 'week']\n",
      "['terms', 'term']\n",
      "['total', 'total']\n",
      "['weekly', 'weekli']\n",
      "['sales', 'sale']\n",
      "['quantities', 'quantiti']\n",
      "['first', 'first']\n",
      "['extract', 'extract']\n",
      "['year', 'year']\n",
      "['week', 'week']\n",
      "['information', 'inform']\n",
      "['date', 'date']\n",
      "['column', 'column']\n",
      "['use', 'use']\n",
      "['aggregation', 'aggreg']\n",
      "['sum', 'sum']\n",
      "['function', 'function']\n",
      "['used', 'use']\n",
      "['calculate', 'calcul']\n",
      "['total', 'total']\n",
      "['sales', 'sale']\n",
      "['quantities', 'quantiti']\n",
      "['sales', 'sale']\n",
      "['table', 'tabl']\n",
      "['find', 'find']\n",
      "['number', 'number']\n",
      "['unique', 'uniqu']\n",
      "['items', 'item']\n",
      "['sold', 'sold']\n",
      "['month', 'month']\n",
      "['terms', 'term']\n",
      "['related', 'relat']\n",
      "['database', 'databas']\n",
      "['schema', 'schema']\n",
      "['design', 'design']\n",
      "['normalization', 'normal']\n",
      "['denormalization', 'denorm']\n",
      "['aim', 'aim']\n",
      "['optimize', 'optim']\n",
      "['different', 'differ']\n",
      "['metrics', 'metric']\n",
      "['goal', 'goal']\n",
      "['normalization', 'normal']\n",
      "['reduce', 'reduc']\n",
      "['data', 'data']\n",
      "['redundancy', 'redund']\n",
      "['inconsistency', 'inconsist']\n",
      "['increasing', 'increas']\n",
      "['number', 'number']\n",
      "['tables', 'tabl']\n",
      "['hand', 'hand']\n",
      "['denormalization', 'denorm']\n",
      "['aims', 'aim']\n",
      "['speed', 'speed']\n",
      "['query', 'queri']\n",
      "['execution', 'execut']\n",
      "['denormalization', 'denorm']\n",
      "['decreases', 'decreas']\n",
      "['number', 'number']\n",
      "['tables', 'tabl']\n",
      "['time', 'time']\n",
      "['adds', 'add']\n",
      "['redundancy', 'redund']\n",
      "['challenging', 'challeng']\n",
      "['task', 'task']\n",
      "['become', 'becom']\n",
      "['data', 'data']\n",
      "['scientist', 'scientist']\n",
      "['requires', 'requir']\n",
      "['time', 'time']\n",
      "['effort', 'effort']\n",
      "['dedication', 'dedic']\n",
      "['without', 'without']\n",
      "['prior', 'prior']\n",
      "['job', 'job']\n",
      "['experience', 'experi']\n",
      "['process', 'process']\n",
      "['gets', 'get']\n",
      "['harder', 'harder']\n",
      "['interviews', 'interview']\n",
      "['important', 'import']\n",
      "['demonstrate', 'demonstr']\n",
      "['skills', 'skill']\n",
      "['article', 'articl']\n",
      "['covered', 'cover']\n",
      "['10', '10']\n",
      "['questions', 'question']\n",
      "['likely', 'like']\n",
      "['encounter', 'encount']\n",
      "['data', 'data']\n",
      "['scientist', 'scientist']\n",
      "['interview', 'interview']\n",
      "['thank', 'thank']\n",
      "['reading', 'read']\n",
      "['please', 'pleas']\n",
      "['let', 'let']\n",
      "['know', 'know']\n",
      "['feedback', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for token in tokens_no_sw_nltk_custom:\n",
    "    root = ps.stem(token)\n",
    "    print([token, root])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How did it do? Do you think the Porter stemmer is useful for your work? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With stemming, the number of distinct values decrease to 288. Tokens can be aggregated to less groups for a faster analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Undo the Porter stemmer and instead apply the Lancaster stemmer to your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popularity', 'popul']\n",
      "['data', 'dat']\n",
      "['science', 'sci']\n",
      "['attracts', 'attract']\n",
      "['lot', 'lot']\n",
      "['people', 'peopl']\n",
      "['wide', 'wid']\n",
      "['range', 'rang']\n",
      "['professions', 'profess']\n",
      "['make', 'mak']\n",
      "['career', 'car']\n",
      "['change', 'chang']\n",
      "['goal', 'goal']\n",
      "['becoming', 'becom']\n",
      "['data', 'dat']\n",
      "['scientist', 'sci']\n",
      "['despite', 'despit']\n",
      "['high', 'high']\n",
      "['demand', 'demand']\n",
      "['data', 'dat']\n",
      "['scientists', 'sci']\n",
      "['highly', 'high']\n",
      "['challenging', 'challeng']\n",
      "['task', 'task']\n",
      "['find', 'find']\n",
      "['first', 'first']\n",
      "['job', 'job']\n",
      "['unless', 'unless']\n",
      "['solid', 'solid']\n",
      "['prior', 'pri']\n",
      "['job', 'job']\n",
      "['experience', 'expery']\n",
      "['interviews', 'interview']\n",
      "['show', 'show']\n",
      "['skills', 'skil']\n",
      "['impress', 'impress']\n",
      "['potential', 'pot']\n",
      "['employer', 'employ']\n",
      "['data', 'dat']\n",
      "['science', 'sci']\n",
      "['interdisciplinary', 'interdisciplin']\n",
      "['field', 'field']\n",
      "['covers', 'cov']\n",
      "['broad', 'broad']\n",
      "['range', 'rang']\n",
      "['topics', 'top']\n",
      "['concepts', 'conceiv']\n",
      "['thus', 'thu']\n",
      "['number', 'numb']\n",
      "['questions', 'quest']\n",
      "['might', 'might']\n",
      "['asked', 'ask']\n",
      "['interview', 'interview']\n",
      "['high', 'high']\n",
      "['however', 'howev']\n",
      "['questions', 'quest']\n",
      "['fundamentals', 'funda']\n",
      "['data', 'dat']\n",
      "['science', 'sci']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['ones', 'on']\n",
      "['want', 'want']\n",
      "['miss', 'miss']\n",
      "['article', 'artic']\n",
      "['go', 'go']\n",
      "['10', '10']\n",
      "['questions', 'quest']\n",
      "['likely', 'lik']\n",
      "['asked', 'ask']\n",
      "['data', 'dat']\n",
      "['scientist', 'sci']\n",
      "['interview', 'interview']\n",
      "['questions', 'quest']\n",
      "['grouped', 'group']\n",
      "['3', '3']\n",
      "['main', 'main']\n",
      "['categories', 'categ']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['python', 'python']\n",
      "['sql', 'sql']\n",
      "['try', 'try']\n",
      "['provide', 'provid']\n",
      "['brief', 'brief']\n",
      "['answer', 'answ']\n",
      "['question', 'quest']\n",
      "['however', 'howev']\n",
      "['suggest', 'suggest']\n",
      "['reading', 'read']\n",
      "['studying', 'study']\n",
      "['one', 'on']\n",
      "['detail', 'detail']\n",
      "['afterwards', 'afterward']\n",
      "['overfitting', 'overfit']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['occurs', 'occ']\n",
      "['model', 'model']\n",
      "['generalized', 'gen']\n",
      "['well', 'wel']\n",
      "['model', 'model']\n",
      "['focused', 'focus']\n",
      "['training', 'train']\n",
      "['set', 'set']\n",
      "['captures', 'capt']\n",
      "['lot', 'lot']\n",
      "['detail', 'detail']\n",
      "['even', 'ev']\n",
      "['noise', 'nois']\n",
      "['training', 'train']\n",
      "['set', 'set']\n",
      "['thus', 'thu']\n",
      "['fails', 'fail']\n",
      "['capture', 'capt']\n",
      "['general', 'gen']\n",
      "['trend', 'trend']\n",
      "['relationships', 'rel']\n",
      "['data', 'dat']\n",
      "['model', 'model']\n",
      "['complex', 'complex']\n",
      "['compared', 'comp']\n",
      "['data', 'dat']\n",
      "['probably', 'prob']\n",
      "['overfitting', 'overfit']\n",
      "['strong', 'strong']\n",
      "['indicator', 'ind']\n",
      "['overfitting', 'overfit']\n",
      "['high', 'high']\n",
      "['difference', 'diff']\n",
      "['accuracy', 'acc']\n",
      "['training', 'train']\n",
      "['test', 'test']\n",
      "['sets', 'set']\n",
      "['overfit', 'overfit']\n",
      "['models', 'model']\n",
      "['usually', 'us']\n",
      "['high', 'high']\n",
      "['accuracy', 'acc']\n",
      "['training', 'train']\n",
      "['set', 'set']\n",
      "['test', 'test']\n",
      "['accuracy', 'acc']\n",
      "['usually', 'us']\n",
      "['unpredictable', 'unpredict']\n",
      "['much', 'much']\n",
      "['lower', 'low']\n",
      "['training', 'train']\n",
      "['accuracy', 'acc']\n",
      "['reduce', 'reduc']\n",
      "['overfitting', 'overfit']\n",
      "['making', 'mak']\n",
      "['model', 'model']\n",
      "['generalized', 'gen']\n",
      "['means', 'mean']\n",
      "['focused', 'focus']\n",
      "['general', 'gen']\n",
      "['trend', 'trend']\n",
      "['rather', 'rath']\n",
      "['specific', 'spec']\n",
      "['details', 'detail']\n",
      "['possible', 'poss']\n",
      "['collecting', 'collect']\n",
      "['data', 'dat']\n",
      "['efficient', 'efficy']\n",
      "['way', 'way']\n",
      "['reduce', 'reduc']\n",
      "['overfitting', 'overfit']\n",
      "['giving', 'giv']\n",
      "['juice', 'juic']\n",
      "['model', 'model']\n",
      "['material', 'mat']\n",
      "['learn', 'learn']\n",
      "['data', 'dat']\n",
      "['always', 'alway']\n",
      "['valuable', 'valu']\n",
      "['especially', 'espec']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['models', 'model']\n",
      "['another', 'anoth']\n",
      "['method', 'method']\n",
      "['reduce', 'reduc']\n",
      "['overfitting', 'overfit']\n",
      "['reduce', 'reduc']\n",
      "['complexity', 'complex']\n",
      "['model', 'model']\n",
      "['model', 'model']\n",
      "['complex', 'complex']\n",
      "['given', 'giv']\n",
      "['task', 'task']\n",
      "['likely', 'lik']\n",
      "['result', 'result']\n",
      "['overfitting', 'overfit']\n",
      "['cases', 'cas']\n",
      "['look', 'look']\n",
      "['simpler', 'simpl']\n",
      "['models', 'model']\n",
      "['mentioned', 'ment']\n",
      "['main', 'main']\n",
      "['reason', 'reason']\n",
      "['overfitting', 'overfit']\n",
      "['model', 'model']\n",
      "['complex', 'complex']\n",
      "['necessary', 'necess']\n",
      "['regularization', 'regul']\n",
      "['method', 'method']\n",
      "['reducing', 'reduc']\n",
      "['model', 'model']\n",
      "['complexity', 'complex']\n",
      "['penalizing', 'pen']\n",
      "['higher', 'high']\n",
      "['terms', 'term']\n",
      "['model', 'model']\n",
      "['addition', 'addit']\n",
      "['regularization', 'regul']\n",
      "['term', 'term']\n",
      "['model', 'model']\n",
      "['tries', 'tri']\n",
      "['minimize', 'minim']\n",
      "['loss', 'loss']\n",
      "['complexity', 'complex']\n",
      "['two', 'two']\n",
      "['main', 'main']\n",
      "['types', 'typ']\n",
      "['regularization', 'regul']\n",
      "['l1', 'l1']\n",
      "['l2', 'l2']\n",
      "['regularization', 'regul']\n",
      "['l1', 'l1']\n",
      "['regularization', 'regul']\n",
      "['subtracts', 'subtract']\n",
      "['small', 'smal']\n",
      "['amount', 'amount']\n",
      "['weights', 'weight']\n",
      "['uninformative', 'uninform']\n",
      "['features', 'feat']\n",
      "['iteration', 'it']\n",
      "['thus', 'thu']\n",
      "['causes', 'caus']\n",
      "['weights', 'weight']\n",
      "['eventually', 'ev']\n",
      "['become', 'becom']\n",
      "['zero', 'zero']\n",
      "['hand', 'hand']\n",
      "['l2', 'l2']\n",
      "['regularization', 'regul']\n",
      "['removes', 'remov']\n",
      "['small', 'smal']\n",
      "['percentage', 'perc']\n",
      "['weights', 'weight']\n",
      "['iteration', 'it']\n",
      "['weights', 'weight']\n",
      "['get', 'get']\n",
      "['closer', 'clos']\n",
      "['zero', 'zero']\n",
      "['never', 'nev']\n",
      "['actually', 'act']\n",
      "['become', 'becom']\n",
      "['0', '0']\n",
      "['machine', 'machin']\n",
      "['learning', 'learn']\n",
      "['tasks', 'task']\n",
      "['classification', 'class']\n",
      "['supervised', 'superv']\n",
      "['learning', 'learn']\n",
      "['task', 'task']\n",
      "['labelled', 'label']\n",
      "['observations', 'observ']\n",
      "['(', '(']\n",
      "['e', 'e']\n",
      "['data', 'dat']\n",
      "['points', 'point']\n",
      "[')', ')']\n",
      "['train', 'train']\n",
      "['model', 'model']\n",
      "['labelled', 'label']\n",
      "['data', 'dat']\n",
      "['expect', 'expect']\n",
      "['predict', 'predict']\n",
      "['labels', 'label']\n",
      "['new', 'new']\n",
      "['data', 'dat']\n",
      "['instance', 'inst']\n",
      "['spam', 'spam']\n",
      "['email', 'email']\n",
      "['detection', 'detect']\n",
      "['classification', 'class']\n",
      "['task', 'task']\n",
      "['provide', 'provid']\n",
      "['model', 'model']\n",
      "['several', 'sev']\n",
      "['emails', 'email']\n",
      "['marked', 'mark']\n",
      "['spam', 'spam']\n",
      "['spam', 'spam']\n",
      "['model', 'model']\n",
      "['trained', 'train']\n",
      "['emails', 'email']\n",
      "['evaluate', 'evalu']\n",
      "['new', 'new']\n",
      "['emails', 'email']\n",
      "['appropriately', 'appropry']\n",
      "['clustering', 'clust']\n",
      "['unsupervised', 'unsuperv']\n",
      "['learning', 'learn']\n",
      "['task', 'task']\n",
      "['observations', 'observ']\n",
      "['labels', 'label']\n",
      "['model', 'model']\n",
      "['expected', 'expect']\n",
      "['evaluate', 'evalu']\n",
      "['observations', 'observ']\n",
      "['group', 'group']\n",
      "['clusters', 'clust']\n",
      "['similar', 'simil']\n",
      "['observations', 'observ']\n",
      "['placed', 'plac']\n",
      "['cluster', 'clust']\n",
      "['optimal', 'optim']\n",
      "['case', 'cas']\n",
      "['observations', 'observ']\n",
      "['cluster', 'clust']\n",
      "['close', 'clos']\n",
      "['possible', 'poss']\n",
      "['different', 'diff']\n",
      "['clusters', 'clust']\n",
      "['far', 'far']\n",
      "['apart', 'apart']\n",
      "['possible', 'poss']\n",
      "['example', 'exampl']\n",
      "['clustering', 'clust']\n",
      "['task', 'task']\n",
      "['would', 'would']\n",
      "['grouping', 'group']\n",
      "['customers', 'custom']\n",
      "['based', 'bas']\n",
      "['shopping', 'shop']\n",
      "['behavior', 'behavy']\n",
      "['built-in', 'built-in']\n",
      "['data', 'dat']\n",
      "['structures', 'structures']\n",
      "['crucial', 'cruc']\n",
      "['importance', 'import']\n",
      "['thus', 'thu']\n",
      "['familiar', 'famili']\n",
      "['interact', 'interact']\n",
      "['list', 'list']\n",
      "['dictionary', 'dict']\n",
      "['set', 'set']\n",
      "['tuple', 'tupl']\n",
      "['4', '4']\n",
      "['main', 'main']\n",
      "['built-in', 'built-in']\n",
      "['data', 'dat']\n",
      "['structures', 'structures']\n",
      "['python', 'python']\n",
      "['main', 'main']\n",
      "['difference', 'diff']\n",
      "['lists', 'list']\n",
      "['tuples', 'tupl']\n",
      "['mutability', 'mut']\n",
      "['lists', 'list']\n",
      "['mutable', 'mut']\n",
      "['manipulate', 'manip']\n",
      "['adding', 'ad']\n",
      "['removing', 'remov']\n",
      "['items', 'item']\n",
      "['hand', 'hand']\n",
      "['tuples', 'tupl']\n",
      "['immutable', 'immut']\n",
      "['although', 'although']\n",
      "['access', 'access']\n",
      "['element', 'el']\n",
      "['tuple', 'tupl']\n",
      "['modify', 'mod']\n",
      "['content', 'cont']\n",
      "['one', 'on']\n",
      "['important', 'import']\n",
      "['point', 'point']\n",
      "['mention', 'ment']\n",
      "['although', 'although']\n",
      "['tuples', 'tupl']\n",
      "['immutable', 'immut']\n",
      "['contain', 'contain']\n",
      "['mutable', 'mut']\n",
      "['elements', 'el']\n",
      "['lists', 'list']\n",
      "['sets', 'set']\n",
      "['let', 'let']\n",
      "['’', '’']\n",
      "['example', 'exampl']\n",
      "['demonstrate', 'demonst']\n",
      "['main', 'main']\n",
      "['difference', 'diff']\n",
      "['lists', 'list']\n",
      "['sets', 'set']\n",
      "['notice', 'not']\n",
      "['resulting', 'result']\n",
      "['objects', 'object']\n",
      "['list', 'list']\n",
      "['contains', 'contain']\n",
      "['characters', 'charact']\n",
      "['string', 'string']\n",
      "['whereas', 'wherea']\n",
      "['set', 'set']\n",
      "['contains', 'contain']\n",
      "['unique', 'un']\n",
      "['values', 'valu']\n",
      "['another', 'anoth']\n",
      "['difference', 'diff']\n",
      "['characters', 'charact']\n",
      "['list', 'list']\n",
      "['ordered', 'ord']\n",
      "['based', 'bas']\n",
      "['location', 'loc']\n",
      "['string', 'string']\n",
      "['however', 'howev']\n",
      "['order', 'ord']\n",
      "['associated', 'assocy']\n",
      "['characters', 'charact']\n",
      "['set', 'set']\n",
      "['table', 'tabl']\n",
      "['summarizes', 'summ']\n",
      "['main', 'main']\n",
      "['characteristics', 'charact']\n",
      "['lists', 'list']\n",
      "['tuples', 'tupl']\n",
      "['sets', 'set']\n",
      "['dictionary', 'dict']\n",
      "['python', 'python']\n",
      "['collection', 'collect']\n",
      "['key-value', 'key-value']\n",
      "['pairs', 'pair']\n",
      "['similar', 'simil']\n",
      "['list', 'list']\n",
      "['sense', 'sens']\n",
      "['item', 'item']\n",
      "['list', 'list']\n",
      "['associated', 'assocy']\n",
      "['index', 'index']\n",
      "['starting', 'start']\n",
      "['0', '0']\n",
      "['dictionary', 'dict']\n",
      "['keys', 'key']\n",
      "['index', 'index']\n",
      "['thus', 'thu']\n",
      "['access', 'access']\n",
      "['value', 'valu']\n",
      "['using', 'us']\n",
      "['key', 'key']\n",
      "['keys', 'key']\n",
      "['dictionary', 'dict']\n",
      "['unique', 'un']\n",
      "['makes', 'mak']\n",
      "['sense', 'sens']\n",
      "['act', 'act']\n",
      "['like', 'lik']\n",
      "['address', 'address']\n",
      "['values', 'valu']\n",
      "['sql', 'sql']\n",
      "['extremely', 'extrem']\n",
      "['important', 'import']\n",
      "['skill', 'skil']\n",
      "['data', 'dat']\n",
      "['scientists', 'sci']\n",
      "['quite', 'quit']\n",
      "['number', 'numb']\n",
      "['companies', 'company']\n",
      "['store', 'stor']\n",
      "['data', 'dat']\n",
      "['relational', 'rel']\n",
      "['database', 'databas']\n",
      "['sql', 'sql']\n",
      "['needed', 'nee']\n",
      "['interact', 'interact']\n",
      "['relational', 'rel']\n",
      "['databases', 'databas']\n",
      "['probably', 'prob']\n",
      "['asked', 'ask']\n",
      "['question', 'quest']\n",
      "['involves', 'involv']\n",
      "['writing', 'writ']\n",
      "['query', 'query']\n",
      "['perform', 'perform']\n",
      "['specific', 'spec']\n",
      "['task', 'task']\n",
      "['might', 'might']\n",
      "['also', 'also']\n",
      "['asked', 'ask']\n",
      "['question', 'quest']\n",
      "['general', 'gen']\n",
      "['database', 'databas']\n",
      "['knowledge', 'knowledg']\n",
      "['consider', 'consid']\n",
      "['sales', 'sal']\n",
      "['table', 'tabl']\n",
      "['contains', 'contain']\n",
      "['daily', 'dai']\n",
      "['sales', 'sal']\n",
      "['quantities', 'quant']\n",
      "['products', 'produc']\n",
      "['find', 'find']\n",
      "['top', 'top']\n",
      "['5', '5']\n",
      "['weeks', 'week']\n",
      "['terms', 'term']\n",
      "['total', 'tot']\n",
      "['weekly', 'week']\n",
      "['sales', 'sal']\n",
      "['quantities', 'quant']\n",
      "['first', 'first']\n",
      "['extract', 'extract']\n",
      "['year', 'year']\n",
      "['week', 'week']\n",
      "['information', 'inform']\n",
      "['date', 'dat']\n",
      "['column', 'column']\n",
      "['use', 'us']\n",
      "['aggregation', 'aggreg']\n",
      "['sum', 'sum']\n",
      "['function', 'funct']\n",
      "['used', 'us']\n",
      "['calculate', 'calc']\n",
      "['total', 'tot']\n",
      "['sales', 'sal']\n",
      "['quantities', 'quant']\n",
      "['sales', 'sal']\n",
      "['table', 'tabl']\n",
      "['find', 'find']\n",
      "['number', 'numb']\n",
      "['unique', 'un']\n",
      "['items', 'item']\n",
      "['sold', 'sold']\n",
      "['month', 'mon']\n",
      "['terms', 'term']\n",
      "['related', 'rel']\n",
      "['database', 'databas']\n",
      "['schema', 'schema']\n",
      "['design', 'design']\n",
      "['normalization', 'norm']\n",
      "['denormalization', 'denorm']\n",
      "['aim', 'aim']\n",
      "['optimize', 'optim']\n",
      "['different', 'diff']\n",
      "['metrics', 'met']\n",
      "['goal', 'goal']\n",
      "['normalization', 'norm']\n",
      "['reduce', 'reduc']\n",
      "['data', 'dat']\n",
      "['redundancy', 'redund']\n",
      "['inconsistency', 'inconsist']\n",
      "['increasing', 'increas']\n",
      "['number', 'numb']\n",
      "['tables', 'tabl']\n",
      "['hand', 'hand']\n",
      "['denormalization', 'denorm']\n",
      "['aims', 'aim']\n",
      "['speed', 'spee']\n",
      "['query', 'query']\n",
      "['execution', 'execut']\n",
      "['denormalization', 'denorm']\n",
      "['decreases', 'decreas']\n",
      "['number', 'numb']\n",
      "['tables', 'tabl']\n",
      "['time', 'tim']\n",
      "['adds', 'ad']\n",
      "['redundancy', 'redund']\n",
      "['challenging', 'challeng']\n",
      "['task', 'task']\n",
      "['become', 'becom']\n",
      "['data', 'dat']\n",
      "['scientist', 'sci']\n",
      "['requires', 'requir']\n",
      "['time', 'tim']\n",
      "['effort', 'effort']\n",
      "['dedication', 'ded']\n",
      "['without', 'without']\n",
      "['prior', 'pri']\n",
      "['job', 'job']\n",
      "['experience', 'expery']\n",
      "['process', 'process']\n",
      "['gets', 'get']\n",
      "['harder', 'hard']\n",
      "['interviews', 'interview']\n",
      "['important', 'import']\n",
      "['demonstrate', 'demonst']\n",
      "['skills', 'skil']\n",
      "['article', 'artic']\n",
      "['covered', 'cov']\n",
      "['10', '10']\n",
      "['questions', 'quest']\n",
      "['likely', 'lik']\n",
      "['encounter', 'encount']\n",
      "['data', 'dat']\n",
      "['scientist', 'sci']\n",
      "['interview', 'interview']\n",
      "['thank', 'thank']\n",
      "['reading', 'read']\n",
      "['please', 'pleas']\n",
      "['let', 'let']\n",
      "['know', 'know']\n",
      "['feedback', 'feedback']\n",
      "There are 274 terms in my corpus.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "myset_stem_lan = set()\n",
    "for token in tokens_no_sw_nltk_custom:\n",
    "    root = ls.stem(token)\n",
    "    myset_stem_lan.add(root)\n",
    "    print([token, root])\n",
    "\n",
    "print(f'There are {len(myset_stem_lan)} terms in my corpus.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) How did the Lancaster stemmer do? Do you think it's more or less useful for your work than the Porter stemmer? Briefly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster stemmer lower down the number of unique tokens to 274."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the difference between two results, we find PortStemmer result has following result that not in Lancaster stemmer: \n",
      "{'prior', 'experi', 'base', 'type', 'date', 'harder', 'regular', 'one', 'place', 'manipul', 'time', 'materi', 'skill', 'normal', 'unsupervis', 'use', 'cover', 'daili', 'behavior', 'concept', 'never', 'possibl', 'scientist', 'gener', 'demonstr', 'notic', 'iter', 'indic', 'career', 'element', 'wide', 'like', 'compar', 'case', 'relat', 'number', 'lower', 'question', 'product', 'categori', 'uniqu', 'relationship', 'speed', 'fundament', 'especi', 'even', 'sever', 'captur', 'interdisciplinari', 'answer', 'rather', 'effici', 'mention', 'associ', 'make', 'structur', 'metric', 'topic', 'summar', 'scienc', 'data', 'key-valu', 'mutabl', 'actual', 'dedic', 'characterist', 'given', 'highli', 'closer', 'classif', 'function', 'give', 'total', 'penal', 'necessari', 'potenti', 'differ', 'queri', 'compani', 'locat', 'add', 'crucial', 'valuabl', 'cluster', 'articl', 'probabl', 'dictionari', 'higher', 'occur', 'eventu', 'studi', 'supervis', 'small', 'quantiti', 'month', 'weekli', 'order', 'specif', 'simpler', 'familiar', 'modifi', 'instanc', 'calcul', 'write', 'close', 'accuraci', 'popular', 'store', 'usual', 'content', 'similar', 'well', 'featur', 'appropri', 'sale', 'need', 'percentag'}\n",
      "Lancaster stemmer result has following result that not in PortStemmer: \n",
      "{'popul', 'query', 'ord', 'numb', 'bas', 'interdisciplin', 'rel', 'regul', 'categ', 'class', 'structures', 'acc', 'manip', 'pri', 'stor', 'typ', 'espec', 'sci', 'sal', 'cruc', 'plac', 'superv', 'smal', 'try', 'on', 'dict', 'tot', 'not', 'pen', 'assocy', 'inst', 'poss', 'behavy', 'rath', 'demonst', 'funct', 'skil', 'nev', 'hard', 'occ', 'us', 'ded', 'funda', 'nee', 'tim', 'lik', 'company', 'ment', 'mak', 'study', 'feat', 'conceiv', 'dat', 'mon', 'it', 'car', 'mod', 'summ', 'pot', 'calc', 'el', 'diff', 'dai', 'prob', 'unsuperv', 'simil', 'ev', 'clust', 'cov', 'cas', 'sev', 'key-value', 'necess', 'ind', 'clos', 'efficy', 'appropry', 'cont', 'comp', 'loc', 'low', 'quest', 'wel', 'simpl', 'capt', 'perc', 'giv', 'gen', 'artic', 'mut', 'wid', 'met', 'expery', 'quant', 'writ', 'mat', 'norm', 'spec', 'answ', 'produc', 'famili', 'un', 'spee'}\n"
     ]
    }
   ],
   "source": [
    "print(f'Comparing the difference between two results, we find PortStemmer result has following result that not in Lancaster stemmer: \\n{myset_stem_por - myset_stem_lan}')\n",
    "print(f'Lancaster stemmer result has following result that not in PortStemmer: \\n{myset_stem_lan - myset_stem_por}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I personally prefer the result get from the PortStemmer, because those are more readable to me. For example, the word \\\"fundamental\\\" is written as \\\"fundament\\\" in PortStemmer result, but \\\"funda\\\" in LancasterStemmer result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Last but (well, maybe) not least ... Lemmatization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Go on, Lemmatize! You may do so in any way you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yuxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[popularity, popularity]\n",
      "[data, data]\n",
      "[science, science]\n",
      "[attracts, attracts]\n",
      "[lot, lot]\n",
      "[people, people]\n",
      "[wide, wide]\n",
      "[range, range]\n",
      "[professions, profession]\n",
      "[make, make]\n",
      "[career, career]\n",
      "[change, change]\n",
      "[goal, goal]\n",
      "[becoming, becoming]\n",
      "[data, data]\n",
      "[scientist, scientist]\n",
      "[despite, despite]\n",
      "[high, high]\n",
      "[demand, demand]\n",
      "[data, data]\n",
      "[scientists, scientist]\n",
      "[highly, highly]\n",
      "[challenging, challenging]\n",
      "[task, task]\n",
      "[find, find]\n",
      "[first, first]\n",
      "[job, job]\n",
      "[unless, unless]\n",
      "[solid, solid]\n",
      "[prior, prior]\n",
      "[job, job]\n",
      "[experience, experience]\n",
      "[interviews, interview]\n",
      "[show, show]\n",
      "[skills, skill]\n",
      "[impress, impress]\n",
      "[potential, potential]\n",
      "[employer, employer]\n",
      "[data, data]\n",
      "[science, science]\n",
      "[interdisciplinary, interdisciplinary]\n",
      "[field, field]\n",
      "[covers, cover]\n",
      "[broad, broad]\n",
      "[range, range]\n",
      "[topics, topic]\n",
      "[concepts, concept]\n",
      "[thus, thus]\n",
      "[number, number]\n",
      "[questions, question]\n",
      "[might, might]\n",
      "[asked, asked]\n",
      "[interview, interview]\n",
      "[high, high]\n",
      "[however, however]\n",
      "[questions, question]\n",
      "[fundamentals, fundamental]\n",
      "[data, data]\n",
      "[science, science]\n",
      "[machine, machine]\n",
      "[learning, learning]\n",
      "[ones, one]\n",
      "[want, want]\n",
      "[miss, miss]\n",
      "[article, article]\n",
      "[go, go]\n",
      "[10, 10]\n",
      "[questions, question]\n",
      "[likely, likely]\n",
      "[asked, asked]\n",
      "[data, data]\n",
      "[scientist, scientist]\n",
      "[interview, interview]\n",
      "[questions, question]\n",
      "[grouped, grouped]\n",
      "[3, 3]\n",
      "[main, main]\n",
      "[categories, category]\n",
      "[machine, machine]\n",
      "[learning, learning]\n",
      "[python, python]\n",
      "[sql, sql]\n",
      "[try, try]\n",
      "[provide, provide]\n",
      "[brief, brief]\n",
      "[answer, answer]\n",
      "[question, question]\n",
      "[however, however]\n",
      "[suggest, suggest]\n",
      "[reading, reading]\n",
      "[studying, studying]\n",
      "[one, one]\n",
      "[detail, detail]\n",
      "[afterwards, afterwards]\n",
      "[overfitting, overfitting]\n",
      "[machine, machine]\n",
      "[learning, learning]\n",
      "[occurs, occurs]\n",
      "[model, model]\n",
      "[generalized, generalized]\n",
      "[well, well]\n",
      "[model, model]\n",
      "[focused, focused]\n",
      "[training, training]\n",
      "[set, set]\n",
      "[captures, capture]\n",
      "[lot, lot]\n",
      "[detail, detail]\n",
      "[even, even]\n",
      "[noise, noise]\n",
      "[training, training]\n",
      "[set, set]\n",
      "[thus, thus]\n",
      "[fails, fails]\n",
      "[capture, capture]\n",
      "[general, general]\n",
      "[trend, trend]\n",
      "[relationships, relationship]\n",
      "[data, data]\n",
      "[model, model]\n",
      "[complex, complex]\n",
      "[compared, compared]\n",
      "[data, data]\n",
      "[probably, probably]\n",
      "[overfitting, overfitting]\n",
      "[strong, strong]\n",
      "[indicator, indicator]\n",
      "[overfitting, overfitting]\n",
      "[high, high]\n",
      "[difference, difference]\n",
      "[accuracy, accuracy]\n",
      "[training, training]\n",
      "[test, test]\n",
      "[sets, set]\n",
      "[overfit, overfit]\n",
      "[models, model]\n",
      "[usually, usually]\n",
      "[high, high]\n",
      "[accuracy, accuracy]\n",
      "[training, training]\n",
      "[set, set]\n",
      "[test, test]\n",
      "[accuracy, accuracy]\n",
      "[usually, usually]\n",
      "[unpredictable, unpredictable]\n",
      "[much, much]\n",
      "[lower, lower]\n",
      "[training, training]\n",
      "[accuracy, accuracy]\n",
      "[reduce, reduce]\n",
      "[overfitting, overfitting]\n",
      "[making, making]\n",
      "[model, model]\n",
      "[generalized, generalized]\n",
      "[means, mean]\n",
      "[focused, focused]\n",
      "[general, general]\n",
      "[trend, trend]\n",
      "[rather, rather]\n",
      "[specific, specific]\n",
      "[details, detail]\n",
      "[possible, possible]\n",
      "[collecting, collecting]\n",
      "[data, data]\n",
      "[efficient, efficient]\n",
      "[way, way]\n",
      "[reduce, reduce]\n",
      "[overfitting, overfitting]\n",
      "[giving, giving]\n",
      "[juice, juice]\n",
      "[model, model]\n",
      "[material, material]\n",
      "[learn, learn]\n",
      "[data, data]\n",
      "[always, always]\n",
      "[valuable, valuable]\n",
      "[especially, especially]\n",
      "[machine, machine]\n",
      "[learning, learning]\n",
      "[models, model]\n",
      "[another, another]\n",
      "[method, method]\n",
      "[reduce, reduce]\n",
      "[overfitting, overfitting]\n",
      "[reduce, reduce]\n",
      "[complexity, complexity]\n",
      "[model, model]\n",
      "[model, model]\n",
      "[complex, complex]\n",
      "[given, given]\n",
      "[task, task]\n",
      "[likely, likely]\n",
      "[result, result]\n",
      "[overfitting, overfitting]\n",
      "[cases, case]\n",
      "[look, look]\n",
      "[simpler, simpler]\n",
      "[models, model]\n",
      "[mentioned, mentioned]\n",
      "[main, main]\n",
      "[reason, reason]\n",
      "[overfitting, overfitting]\n",
      "[model, model]\n",
      "[complex, complex]\n",
      "[necessary, necessary]\n",
      "[regularization, regularization]\n",
      "[method, method]\n",
      "[reducing, reducing]\n",
      "[model, model]\n",
      "[complexity, complexity]\n",
      "[penalizing, penalizing]\n",
      "[higher, higher]\n",
      "[terms, term]\n",
      "[model, model]\n",
      "[addition, addition]\n",
      "[regularization, regularization]\n",
      "[term, term]\n",
      "[model, model]\n",
      "[tries, try]\n",
      "[minimize, minimize]\n",
      "[loss, loss]\n",
      "[complexity, complexity]\n",
      "[two, two]\n",
      "[main, main]\n",
      "[types, type]\n",
      "[regularization, regularization]\n",
      "[l1, l1]\n",
      "[l2, l2]\n",
      "[regularization, regularization]\n",
      "[l1, l1]\n",
      "[regularization, regularization]\n",
      "[subtracts, subtracts]\n",
      "[small, small]\n",
      "[amount, amount]\n",
      "[weights, weight]\n",
      "[uninformative, uninformative]\n",
      "[features, feature]\n",
      "[iteration, iteration]\n",
      "[thus, thus]\n",
      "[causes, cause]\n",
      "[weights, weight]\n",
      "[eventually, eventually]\n",
      "[become, become]\n",
      "[zero, zero]\n",
      "[hand, hand]\n",
      "[l2, l2]\n",
      "[regularization, regularization]\n",
      "[removes, remove]\n",
      "[small, small]\n",
      "[percentage, percentage]\n",
      "[weights, weight]\n",
      "[iteration, iteration]\n",
      "[weights, weight]\n",
      "[get, get]\n",
      "[closer, closer]\n",
      "[zero, zero]\n",
      "[never, never]\n",
      "[actually, actually]\n",
      "[become, become]\n",
      "[0, 0]\n",
      "[machine, machine]\n",
      "[learning, learning]\n",
      "[tasks, task]\n",
      "[classification, classification]\n",
      "[supervised, supervised]\n",
      "[learning, learning]\n",
      "[task, task]\n",
      "[labelled, labelled]\n",
      "[observations, observation]\n",
      "[(, (]\n",
      "[e, e]\n",
      "[data, data]\n",
      "[points, point]\n",
      "[), )]\n",
      "[train, train]\n",
      "[model, model]\n",
      "[labelled, labelled]\n",
      "[data, data]\n",
      "[expect, expect]\n",
      "[predict, predict]\n",
      "[labels, label]\n",
      "[new, new]\n",
      "[data, data]\n",
      "[instance, instance]\n",
      "[spam, spam]\n",
      "[email, email]\n",
      "[detection, detection]\n",
      "[classification, classification]\n",
      "[task, task]\n",
      "[provide, provide]\n",
      "[model, model]\n",
      "[several, several]\n",
      "[emails, email]\n",
      "[marked, marked]\n",
      "[spam, spam]\n",
      "[spam, spam]\n",
      "[model, model]\n",
      "[trained, trained]\n",
      "[emails, email]\n",
      "[evaluate, evaluate]\n",
      "[new, new]\n",
      "[emails, email]\n",
      "[appropriately, appropriately]\n",
      "[clustering, clustering]\n",
      "[unsupervised, unsupervised]\n",
      "[learning, learning]\n",
      "[task, task]\n",
      "[observations, observation]\n",
      "[labels, label]\n",
      "[model, model]\n",
      "[expected, expected]\n",
      "[evaluate, evaluate]\n",
      "[observations, observation]\n",
      "[group, group]\n",
      "[clusters, cluster]\n",
      "[similar, similar]\n",
      "[observations, observation]\n",
      "[placed, placed]\n",
      "[cluster, cluster]\n",
      "[optimal, optimal]\n",
      "[case, case]\n",
      "[observations, observation]\n",
      "[cluster, cluster]\n",
      "[close, close]\n",
      "[possible, possible]\n",
      "[different, different]\n",
      "[clusters, cluster]\n",
      "[far, far]\n",
      "[apart, apart]\n",
      "[possible, possible]\n",
      "[example, example]\n",
      "[clustering, clustering]\n",
      "[task, task]\n",
      "[would, would]\n",
      "[grouping, grouping]\n",
      "[customers, customer]\n",
      "[based, based]\n",
      "[shopping, shopping]\n",
      "[behavior, behavior]\n",
      "[built-in, built-in]\n",
      "[data, data]\n",
      "[structures, structure]\n",
      "[crucial, crucial]\n",
      "[importance, importance]\n",
      "[thus, thus]\n",
      "[familiar, familiar]\n",
      "[interact, interact]\n",
      "[list, list]\n",
      "[dictionary, dictionary]\n",
      "[set, set]\n",
      "[tuple, tuple]\n",
      "[4, 4]\n",
      "[main, main]\n",
      "[built-in, built-in]\n",
      "[data, data]\n",
      "[structures, structure]\n",
      "[python, python]\n",
      "[main, main]\n",
      "[difference, difference]\n",
      "[lists, list]\n",
      "[tuples, tuples]\n",
      "[mutability, mutability]\n",
      "[lists, list]\n",
      "[mutable, mutable]\n",
      "[manipulate, manipulate]\n",
      "[adding, adding]\n",
      "[removing, removing]\n",
      "[items, item]\n",
      "[hand, hand]\n",
      "[tuples, tuples]\n",
      "[immutable, immutable]\n",
      "[although, although]\n",
      "[access, access]\n",
      "[element, element]\n",
      "[tuple, tuple]\n",
      "[modify, modify]\n",
      "[content, content]\n",
      "[one, one]\n",
      "[important, important]\n",
      "[point, point]\n",
      "[mention, mention]\n",
      "[although, although]\n",
      "[tuples, tuples]\n",
      "[immutable, immutable]\n",
      "[contain, contain]\n",
      "[mutable, mutable]\n",
      "[elements, element]\n",
      "[lists, list]\n",
      "[sets, set]\n",
      "[let, let]\n",
      "[’, ’]\n",
      "[example, example]\n",
      "[demonstrate, demonstrate]\n",
      "[main, main]\n",
      "[difference, difference]\n",
      "[lists, list]\n",
      "[sets, set]\n",
      "[notice, notice]\n",
      "[resulting, resulting]\n",
      "[objects, object]\n",
      "[list, list]\n",
      "[contains, contains]\n",
      "[characters, character]\n",
      "[string, string]\n",
      "[whereas, whereas]\n",
      "[set, set]\n",
      "[contains, contains]\n",
      "[unique, unique]\n",
      "[values, value]\n",
      "[another, another]\n",
      "[difference, difference]\n",
      "[characters, character]\n",
      "[list, list]\n",
      "[ordered, ordered]\n",
      "[based, based]\n",
      "[location, location]\n",
      "[string, string]\n",
      "[however, however]\n",
      "[order, order]\n",
      "[associated, associated]\n",
      "[characters, character]\n",
      "[set, set]\n",
      "[table, table]\n",
      "[summarizes, summarizes]\n",
      "[main, main]\n",
      "[characteristics, characteristic]\n",
      "[lists, list]\n",
      "[tuples, tuples]\n",
      "[sets, set]\n",
      "[dictionary, dictionary]\n",
      "[python, python]\n",
      "[collection, collection]\n",
      "[key-value, key-value]\n",
      "[pairs, pair]\n",
      "[similar, similar]\n",
      "[list, list]\n",
      "[sense, sense]\n",
      "[item, item]\n",
      "[list, list]\n",
      "[associated, associated]\n",
      "[index, index]\n",
      "[starting, starting]\n",
      "[0, 0]\n",
      "[dictionary, dictionary]\n",
      "[keys, key]\n",
      "[index, index]\n",
      "[thus, thus]\n",
      "[access, access]\n",
      "[value, value]\n",
      "[using, using]\n",
      "[key, key]\n",
      "[keys, key]\n",
      "[dictionary, dictionary]\n",
      "[unique, unique]\n",
      "[makes, make]\n",
      "[sense, sense]\n",
      "[act, act]\n",
      "[like, like]\n",
      "[address, address]\n",
      "[values, value]\n",
      "[sql, sql]\n",
      "[extremely, extremely]\n",
      "[important, important]\n",
      "[skill, skill]\n",
      "[data, data]\n",
      "[scientists, scientist]\n",
      "[quite, quite]\n",
      "[number, number]\n",
      "[companies, company]\n",
      "[store, store]\n",
      "[data, data]\n",
      "[relational, relational]\n",
      "[database, database]\n",
      "[sql, sql]\n",
      "[needed, needed]\n",
      "[interact, interact]\n",
      "[relational, relational]\n",
      "[databases, database]\n",
      "[probably, probably]\n",
      "[asked, asked]\n",
      "[question, question]\n",
      "[involves, involves]\n",
      "[writing, writing]\n",
      "[query, query]\n",
      "[perform, perform]\n",
      "[specific, specific]\n",
      "[task, task]\n",
      "[might, might]\n",
      "[also, also]\n",
      "[asked, asked]\n",
      "[question, question]\n",
      "[general, general]\n",
      "[database, database]\n",
      "[knowledge, knowledge]\n",
      "[consider, consider]\n",
      "[sales, sale]\n",
      "[table, table]\n",
      "[contains, contains]\n",
      "[daily, daily]\n",
      "[sales, sale]\n",
      "[quantities, quantity]\n",
      "[products, product]\n",
      "[find, find]\n",
      "[top, top]\n",
      "[5, 5]\n",
      "[weeks, week]\n",
      "[terms, term]\n",
      "[total, total]\n",
      "[weekly, weekly]\n",
      "[sales, sale]\n",
      "[quantities, quantity]\n",
      "[first, first]\n",
      "[extract, extract]\n",
      "[year, year]\n",
      "[week, week]\n",
      "[information, information]\n",
      "[date, date]\n",
      "[column, column]\n",
      "[use, use]\n",
      "[aggregation, aggregation]\n",
      "[sum, sum]\n",
      "[function, function]\n",
      "[used, used]\n",
      "[calculate, calculate]\n",
      "[total, total]\n",
      "[sales, sale]\n",
      "[quantities, quantity]\n",
      "[sales, sale]\n",
      "[table, table]\n",
      "[find, find]\n",
      "[number, number]\n",
      "[unique, unique]\n",
      "[items, item]\n",
      "[sold, sold]\n",
      "[month, month]\n",
      "[terms, term]\n",
      "[related, related]\n",
      "[database, database]\n",
      "[schema, schema]\n",
      "[design, design]\n",
      "[normalization, normalization]\n",
      "[denormalization, denormalization]\n",
      "[aim, aim]\n",
      "[optimize, optimize]\n",
      "[different, different]\n",
      "[metrics, metric]\n",
      "[goal, goal]\n",
      "[normalization, normalization]\n",
      "[reduce, reduce]\n",
      "[data, data]\n",
      "[redundancy, redundancy]\n",
      "[inconsistency, inconsistency]\n",
      "[increasing, increasing]\n",
      "[number, number]\n",
      "[tables, table]\n",
      "[hand, hand]\n",
      "[denormalization, denormalization]\n",
      "[aims, aim]\n",
      "[speed, speed]\n",
      "[query, query]\n",
      "[execution, execution]\n",
      "[denormalization, denormalization]\n",
      "[decreases, decrease]\n",
      "[number, number]\n",
      "[tables, table]\n",
      "[time, time]\n",
      "[adds, add]\n",
      "[redundancy, redundancy]\n",
      "[challenging, challenging]\n",
      "[task, task]\n",
      "[become, become]\n",
      "[data, data]\n",
      "[scientist, scientist]\n",
      "[requires, requires]\n",
      "[time, time]\n",
      "[effort, effort]\n",
      "[dedication, dedication]\n",
      "[without, without]\n",
      "[prior, prior]\n",
      "[job, job]\n",
      "[experience, experience]\n",
      "[process, process]\n",
      "[gets, get]\n",
      "[harder, harder]\n",
      "[interviews, interview]\n",
      "[important, important]\n",
      "[demonstrate, demonstrate]\n",
      "[skills, skill]\n",
      "[article, article]\n",
      "[covered, covered]\n",
      "[10, 10]\n",
      "[questions, question]\n",
      "[likely, likely]\n",
      "[encounter, encounter]\n",
      "[data, data]\n",
      "[scientist, scientist]\n",
      "[interview, interview]\n",
      "[thank, thank]\n",
      "[reading, reading]\n",
      "[please, please]\n",
      "[let, let]\n",
      "[know, know]\n",
      "[feedback, feedback]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for token in tokens_no_sw_nltk_custom:\n",
    "    print(f'[{token}, {wordnet_lemmatizer.lemmatize(token)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Why did you choose the Lemmatizer that you did? (Note: \"It seemed like the easiest one\" is a fine answer for this assignment!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the most basic lemmatizer to use and I only find this lemmatizer class in the nltk document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What do you think is more useful for your work, the winning stemmer from Question 6, or the Lemmatizer? Briefly explain your reasoning. In doing so, please comment on the tradeoffs between the two choices and why you landed where you did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer to use the Lemmatizer, since it not only finds the root, but also returns a full root word. Thus lemmatizer makes the downstream analysis easy to understand and visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summing up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Hopefully you've had a chance to experience, without getting *too* irritated at me for a few touches of tedium in this assignment, that even in this *relatively* uncontroversial pre-processing stage, you had to make a number of choices about how to clean and standardize your text. \n",
    "\n",
    "Overall, which choice was the most difficult to make, and which was the easiest, or most \"obvious\" for you? (Even if none were particularly difficult or particularly easy -- all possible depending on your text -- try to pull out at least the extremes!) Briefly explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find dealing with capitalization is a challenging question. Capitalization format can also deliver information, but capitalization also provides different formats of a individual word that could make analysis difficult. Thus to find the balance between lowering the whole text and keep all capitalization is not easy. There\\'s no universal rule for dealing with capitalization, and each decision is made just for the specific type of corpus.\n",
    "\n",
    "The way I deal with punctuation is pretty straightforwards that's to directly drop the one separating sentences because they do not transfer useful information about the blog content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How confident are you in the choices you've made? In other words, if you were to proceed with analyzing this text, how likely do you think it is that you'd eventually want to go back and tweak (or totally change) some of the decisions you've made? What do you think you'd be most likely to change? Briefly explain your reasoning. If you expect to be 100 percent confident in all your choices in this assignment until the end of time, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I collect more sample texts, I may go ahead to experiment for the best practice for dealing with stemming or lemmatizing. The consideration of change token to word root is for a more smooth, efficient, and reasonable analysis later. Thus, pre-knowledge about how words are roughly used in data science blog could help make a better decision on which stemmer or lemmatizer to use.\n",
    "\n",
    "However, I only inspect one article for this homework, so limited information of words use could be observed, so the choice of using lemmatizer is quite temporary, and is flexible to future change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We haven't learned much (yet!!) about what to actually *do* with text once it's pre-processed, but now that you have it, what do *you* imagine the next step would be in your analysis in order to test (or at least get closer to being ready to test) one or all of the hypotheses you identified in Question 1? Go with your intincts -- I bet there's a technique for it (and if there isn't, well, now you're a future NLP methods developer!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the list of tokens, I imagine next step to be vectorize tokens so that the computer could understand the number and potentially find relationships between tokens using machine learning tecniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Finally, while, again, we haven't *really* done any analysis yet, what's something you've learned about your text from this pre-processing work? No lesson is too large or too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can indeed see the complex side of language from this homework. I never clearly realize how many variations of words, choice of non-letter characters, rules of grammar in a language. This is a challenge when we want to summarize a standardized system that the computer could understand. We human could learn a new language with ongoing exposure of that specific language, but computer don\\' know how to build a readable connection between words easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end! \n",
    "\n",
    "Congratulations on finishing your first assignment, and your first NLP work ever (for most of you, at least)! Don't forget to comment out (not delete!) the code that you decided is not appropriate for your work. In other words, leave the final version of this notebook such that we can run *your* analysis top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
